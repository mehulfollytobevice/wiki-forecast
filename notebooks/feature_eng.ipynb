{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f7c0440-095f-4125-8471-8c0aca830a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f6314a8-6df6-49ef-885a-d1f3f8788267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Non-stationary starting autoregressive parameters found\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Non-invertible starting MA parameters found\")\n",
    "\n",
    "from utils import (\n",
    "    extract_page_components, \n",
    "    extract_main_domain, \n",
    "    get_date_columns, \n",
    "    remove_incomplete_rows, \n",
    "    plot_time_series, \n",
    "    plot_seasonal_decompose,\n",
    "    check_stationarity,\n",
    "    get_cat_to_articles,\n",
    "    filter_articles\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccadddd4-a93d-4292-8db9-46b8fa9bff3e",
   "metadata": {},
   "source": [
    "### Read Data And Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a91ea8f9-4ccf-4b67-a2ef-95a4390ff80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145063, 804)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/train_2.csv')\n",
    "data.rename(columns={\n",
    "    'Page': 'page'\n",
    "}, inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbc354d1-119e-4b2f-81be-3603cebce0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>article</th>\n",
       "      <th>domain</th>\n",
       "      <th>locale</th>\n",
       "      <th>access</th>\n",
       "      <th>agent</th>\n",
       "      <th>main_domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2NE1_zh.wikipedia.org_all-access_spider</td>\n",
       "      <td>2NE1</td>\n",
       "      <td>zh.wikipedia.org</td>\n",
       "      <td>zh</td>\n",
       "      <td>all-access</td>\n",
       "      <td>spider</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2PM_zh.wikipedia.org_all-access_spider</td>\n",
       "      <td>2PM</td>\n",
       "      <td>zh.wikipedia.org</td>\n",
       "      <td>zh</td>\n",
       "      <td>all-access</td>\n",
       "      <td>spider</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3C_zh.wikipedia.org_all-access_spider</td>\n",
       "      <td>3C</td>\n",
       "      <td>zh.wikipedia.org</td>\n",
       "      <td>zh</td>\n",
       "      <td>all-access</td>\n",
       "      <td>spider</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4minute_zh.wikipedia.org_all-access_spider</td>\n",
       "      <td>4minute</td>\n",
       "      <td>zh.wikipedia.org</td>\n",
       "      <td>zh</td>\n",
       "      <td>all-access</td>\n",
       "      <td>spider</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52_Hz_I_Love_You_zh.wikipedia.org_all-access_s...</td>\n",
       "      <td>52_Hz_I_Love_You</td>\n",
       "      <td>zh.wikipedia.org</td>\n",
       "      <td>zh</td>\n",
       "      <td>all-access</td>\n",
       "      <td>spider</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                page           article  \\\n",
       "0            2NE1_zh.wikipedia.org_all-access_spider              2NE1   \n",
       "1             2PM_zh.wikipedia.org_all-access_spider               2PM   \n",
       "2              3C_zh.wikipedia.org_all-access_spider                3C   \n",
       "3         4minute_zh.wikipedia.org_all-access_spider           4minute   \n",
       "4  52_Hz_I_Love_You_zh.wikipedia.org_all-access_s...  52_Hz_I_Love_You   \n",
       "\n",
       "             domain locale      access   agent main_domain  \n",
       "0  zh.wikipedia.org     zh  all-access  spider   wikipedia  \n",
       "1  zh.wikipedia.org     zh  all-access  spider   wikipedia  \n",
       "2  zh.wikipedia.org     zh  all-access  spider   wikipedia  \n",
       "3  zh.wikipedia.org     zh  all-access  spider   wikipedia  \n",
       "4  zh.wikipedia.org     zh  all-access  spider   wikipedia  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_info_df = data[['page']].copy()\n",
    "page_info_df[['article', 'domain', 'locale', 'access', 'agent']] = page_info_df['page'].apply(lambda x: pd.Series(extract_page_components(x)))\n",
    "page_info_df['main_domain'] = page_info_df['domain'].apply(extract_main_domain)\n",
    "page_info_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3fc4557-9288-429a-83cc-8392907e2259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49174"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_info_df['article'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ae791d4-98a0-4f4f-a2e5-bd5ee99ebd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_columns = get_date_columns(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c43dcdd0-30ed-47a6-882e-be4f44624f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115084, 804)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = remove_incomplete_rows(data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "683beeb2-b3bc-4977-ae4d-b2dac8072a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>article</th>\n",
       "      <th>\"Weird_Al\"_Yankovic</th>\n",
       "      <th>'Tis_the_Season</th>\n",
       "      <th>'Tis_the_Season_(Vince_Gill_and_Olivia_Newton-John_album)</th>\n",
       "      <th>(1)_Ceres</th>\n",
       "      <th>(500)_Days_of_Summer</th>\n",
       "      <th>.ca</th>\n",
       "      <th>.xxx</th>\n",
       "      <th>0</th>\n",
       "      <th>0.999…</th>\n",
       "      <th>007:_Координаты_«Скайфолл»</th>\n",
       "      <th>...</th>\n",
       "      <th>龍八夷</th>\n",
       "      <th>龍円愛梨</th>\n",
       "      <th>龍應台</th>\n",
       "      <th>龍抬頭</th>\n",
       "      <th>龍涎香</th>\n",
       "      <th>龐茲騙局</th>\n",
       "      <th>龔嘉欣</th>\n",
       "      <th>龙生九子</th>\n",
       "      <th>대문</th>\n",
       "      <th>［Alexandros］</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-07-01</th>\n",
       "      <td>2547.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>404.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>28048.0</td>\n",
       "      <td>293.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>...</td>\n",
       "      <td>758.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>353.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>466.0</td>\n",
       "      <td>617.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>339.0</td>\n",
       "      <td>10864.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-02</th>\n",
       "      <td>20227.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>31576.0</td>\n",
       "      <td>362.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1272.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>426.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>1101.0</td>\n",
       "      <td>795.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>405.0</td>\n",
       "      <td>11917.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-03</th>\n",
       "      <td>1894.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>293.0</td>\n",
       "      <td>372.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>31695.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1276.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>375.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>658.0</td>\n",
       "      <td>984.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>11443.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-04</th>\n",
       "      <td>1472.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>34658.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>...</td>\n",
       "      <td>899.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>338.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>637.0</td>\n",
       "      <td>856.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>345.0</td>\n",
       "      <td>14064.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-05</th>\n",
       "      <td>1380.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>33736.0</td>\n",
       "      <td>281.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>...</td>\n",
       "      <td>792.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>478.0</td>\n",
       "      <td>879.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>14148.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36562 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "article     \"Weird_Al\"_Yankovic  'Tis_the_Season  \\\n",
       "2015-07-01               2547.0             40.0   \n",
       "2015-07-02              20227.0             35.0   \n",
       "2015-07-03               1894.0             33.0   \n",
       "2015-07-04               1472.0             42.0   \n",
       "2015-07-05               1380.0             19.0   \n",
       "\n",
       "article     'Tis_the_Season_(Vince_Gill_and_Olivia_Newton-John_album)  \\\n",
       "2015-07-01                                               18.0           \n",
       "2015-07-02                                               15.0           \n",
       "2015-07-03                                               22.0           \n",
       "2015-07-04                                               18.0           \n",
       "2015-07-05                                                8.0           \n",
       "\n",
       "article     (1)_Ceres  (500)_Days_of_Summer   .ca     .xxx      0  0.999…  \\\n",
       "2015-07-01      404.0                 273.0   3.0  28048.0  293.0    79.0   \n",
       "2015-07-02      380.0                 318.0   7.0  31576.0  362.0    56.0   \n",
       "2015-07-03      293.0                 372.0  11.0  31695.0  278.0    39.0   \n",
       "2015-07-04      229.0                 299.0   8.0  34658.0  218.0    69.0   \n",
       "2015-07-05      224.0                 398.0   5.0  33736.0  281.0    55.0   \n",
       "\n",
       "article     007:_Координаты_«Скайфолл»  ...     龍八夷   龍円愛梨    龍應台   龍抬頭  \\\n",
       "2015-07-01                       125.0  ...   758.0  169.0  353.0   3.0   \n",
       "2015-07-02                       115.0  ...  1272.0  112.0  426.0   0.0   \n",
       "2015-07-03                       105.0  ...  1276.0  156.0  375.0   4.0   \n",
       "2015-07-04                       155.0  ...   899.0  214.0  338.0   3.0   \n",
       "2015-07-05                       183.0  ...   792.0  190.0  350.0  19.0   \n",
       "\n",
       "article       龍涎香    龐茲騙局    龔嘉欣  龙生九子     대문  ［Alexandros］  \n",
       "2015-07-01  147.0   466.0  617.0  95.0  339.0       10864.0  \n",
       "2015-07-02  112.0  1101.0  795.0  86.0  405.0       11917.0  \n",
       "2015-07-03   76.0   658.0  984.0  87.0  360.0       11443.0  \n",
       "2015-07-04   97.0   637.0  856.0  86.0  345.0       14064.0  \n",
       "2015-07-05  152.0   478.0  879.0  81.0  237.0       14148.0  \n",
       "\n",
       "[5 rows x 36562 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = pd.merge(page_info_df[['page', 'article']], data, on='page', how='inner')\n",
    "merged_df.drop('page', axis=1, inplace=True)\n",
    "df = merged_df.groupby('article')[date_columns].sum().reset_index()\n",
    "df.set_index('article', inplace=True)\n",
    "df = df.T\n",
    "df.index = pd.to_datetime(df.index)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14667b5f-183c-440c-9290-dd2be89f8f05",
   "metadata": {},
   "source": [
    "### Categories From Wikipedia API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f07b2aa-8af5-4269-8809-5aec84454029",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_to_articles = get_cat_to_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b35761ef-2f97-4f89-94ab-6c86532a45d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_to_articles = filter_articles(cat_to_articles, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ce1c63-99ac-4b69-a27a-0dd109fbbe3e",
   "metadata": {},
   "source": [
    "There are 34164 categories given by the Wikipedia API for 49174 articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffea1a2-8cd1-41b8-8337-20038072b95e",
   "metadata": {},
   "source": [
    "### Group the categories into a smaller number of overarching themes or topics [Topic Modeling]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc8e0260-c27d-4224-93cc-f8eae0512931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ajaykarthicksenthilkumar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "['deaths', 'books', 'set', 'lgbt', 'user', 'children', '1992', 'animal', 'characters', '2006']\n",
      "\n",
      "\n",
      "Topic 1:\n",
      "['statements', 'articles', 'unsourced', 'dates', 'use', 'potentially', 'dated', 'containing', 'expanded', 'russian']\n",
      "\n",
      "\n",
      "Topic 2:\n",
      "['stubs', 'births', 'geography', 'sport', 'musical', 'groups', 'biography', 'competitions', 'american', 'poets']\n",
      "\n",
      "\n",
      "Topic 3:\n",
      "['united', 'states', 'summer', 'olympics', 'kingdom', 'described', 'dmy', '2016', 'murders', 'establishments']\n",
      "\n",
      "\n",
      "Topic 4:\n",
      "['actresses', 'actors', 'fictional', 'argentine', 'world', 'sportspeople', 'male', 'video', 'cup', 'expatriate']\n",
      "\n",
      "\n",
      "Topic 5:\n",
      "['albums', '20thcentury', '21stcentury', 'singers', 'women', 'spanish', 'italian', '19thcentury', 'male', '18thcentury']\n",
      "\n",
      "\n",
      "Topic 6:\n",
      "['australia', 'south', 'africa', 'african', 'america', 'london', 'establishments', 'radio', 'jews', '1991']\n",
      "\n",
      "\n",
      "Topic 7:\n",
      "['films', '2010s', 'comedy', 'american', 'drama', '2000s', 'action', '1990s', '1970s', 'thriller']\n",
      "\n",
      "\n",
      "Topic 8:\n",
      "['articles', 'lacking', 'references', 'needing', 'additional', 'sources', 'identifiers', '2011', 'businesspeople', 'reliable']\n",
      "\n",
      "\n",
      "Topic 9:\n",
      "['buildings', 'structures', 'cricketers', 'female', 'cultural', 'ireland', 'asia', 'models', '1989', 'dancers']\n",
      "\n",
      "\n",
      "Topic 10:\n",
      "['players', 'text', 'containing', 'articles', 'companies', 'basketball', 'cs1', 'culture', 'baseball', 'sources']\n",
      "\n",
      "\n",
      "Topic 11:\n",
      "['writers', 'politicians', '21stcentury', '20thcentury', 'women', 'establishments', 'germany', 'places', 'populated', 'journalists']\n",
      "\n",
      "\n",
      "Topic 12:\n",
      "['redirects', 'chinese', 'christian', 'monarchs', 'architecture', 'club', 'polish', 'east', 'austria', 'switzerland']\n",
      "\n",
      "\n",
      "Topic 13:\n",
      "['history', 'florida', 'comics', 'roman', 'establishments', 'california', 'programming', 'shows', '1999', '2000']\n",
      "\n",
      "\n",
      "Topic 14:\n",
      "['new', 'songs', 'york', 'state', 'city', 'austrian', 'establishments', 'jersey', 'league', 'football']\n",
      "\n",
      "\n",
      "Topic 15:\n",
      "['television', 'series', 'american', 'debuts', 'endings', 'animated', 'episodes', 'british', 'district', '2010s']\n",
      "\n",
      "\n",
      "Topic 16:\n",
      "['football', 'links', 'dead', 'external', 'established', 'military', 'association', 'named', 'college', 'clubs']\n",
      "\n",
      "\n",
      "Topic 17:\n",
      "['software', 'using', 'pages', 'directed', 'infobox', 'films', '1995', 'suicides', 'winter', 'carolina']\n",
      "\n",
      "\n",
      "Topic 18:\n",
      "['people', 'descent', 'university', 'games', 'events', 'sports', 'academic', 'alumni', 'american', 'county']\n",
      "\n",
      "\n",
      "Topic 19:\n",
      "['american', 'footballers', 'musicians', 'fiction', 'novels', 'australian', 'science', 'men', 'europe', 'music']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "category_names = list(cat_to_articles.keys())\n",
    "\n",
    "# Preprocessing \n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = str(text)\n",
    "    text = text.replace(\"Category:\", \"\")\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\b\\w{1,2}\\b', '', text)  # Remove short words\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "# Preprocess category names\n",
    "preprocessed_category_names = [preprocess_text(name) for name in category_names]\n",
    "\n",
    "# Vectorize the preprocessed category names using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(preprocessed_category_names)\n",
    "\n",
    "# Apply LDA to identify topics\n",
    "num_topics = 20  \n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(tfidf_matrix)\n",
    "\n",
    "# Get the topic distribution for each category\n",
    "topic_distribution = lda.transform(tfidf_matrix)\n",
    "\n",
    "# Assign each category to the most probable topic\n",
    "category_to_topic = {}\n",
    "for i, category in enumerate(category_names):\n",
    "    topic = np.argmax(topic_distribution[i])\n",
    "    category_to_topic[category] = topic\n",
    "\n",
    "# Group categories by their assigned topic\n",
    "from collections import defaultdict\n",
    "topic_to_categories = defaultdict(list)\n",
    "for category, topic in category_to_topic.items():\n",
    "    topic_to_categories[topic].append(category)\n",
    "\n",
    "# Function to get the top words for each topic\n",
    "def get_top_words(model, feature_names, n_top_words):\n",
    "    top_words = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words[topic_idx] = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "    return top_words\n",
    "\n",
    "# Get the feature names (words) from the TF-IDF vectorizer\n",
    "tf_feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get the top words for each topic\n",
    "top_words = get_top_words(lda, tf_feature_names, 10)\n",
    "\n",
    "# Print the top words for each topic\n",
    "for topic, words in top_words.items():\n",
    "    print(f\"Topic {topic}:\")\n",
    "    print(words)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "399d020c-00c4-4ad1-a74b-ba466e7cf51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_article_topic_mappings(cat_to_articles, category_to_topic):\n",
    "    article_to_topics = defaultdict(list)\n",
    "    topic_to_articles = defaultdict(list)\n",
    "\n",
    "    # Create article_to_topics mapping\n",
    "    for category, articles in cat_to_articles.items():\n",
    "        if category in category_to_topic:\n",
    "            topic = category_to_topic[category]\n",
    "            for article in articles:\n",
    "                article_to_topics[article].append(topic)\n",
    "\n",
    "    # Create topic_to_articles mapping\n",
    "    for article, topics in article_to_topics.items():\n",
    "        for topic in topics:\n",
    "            topic_to_articles[topic].append(article)\n",
    "\n",
    "    return article_to_topics, topic_to_articles\n",
    "\n",
    "article_to_topics, topic_to_articles = create_article_topic_mappings(cat_to_articles, category_to_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73645c29-7229-4e33-837d-ce449e32e554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:17<00:00,  1.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variance</th>\n",
       "      <th>Num_Articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.001247</td>\n",
       "      <td>2474.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.001244</td>\n",
       "      <td>9768.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.001247</td>\n",
       "      <td>9205.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.001245</td>\n",
       "      <td>27386.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.001246</td>\n",
       "      <td>3348.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.001247</td>\n",
       "      <td>7559.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.001246</td>\n",
       "      <td>5814.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.001245</td>\n",
       "      <td>6190.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.001246</td>\n",
       "      <td>6235.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.001247</td>\n",
       "      <td>4912.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.001247</td>\n",
       "      <td>2760.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.001247</td>\n",
       "      <td>3482.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.001247</td>\n",
       "      <td>6801.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.001247</td>\n",
       "      <td>2726.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.001247</td>\n",
       "      <td>4864.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.001247</td>\n",
       "      <td>3255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.001247</td>\n",
       "      <td>1811.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.001247</td>\n",
       "      <td>1871.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.001247</td>\n",
       "      <td>2282.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.001247</td>\n",
       "      <td>2003.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Variance  Num_Articles\n",
       "13  1.001247        2474.0\n",
       "16  1.001244        9768.0\n",
       "1   1.001247        9205.0\n",
       "8   1.001245       27386.0\n",
       "0   1.001246        3348.0\n",
       "10  1.001247        7559.0\n",
       "5   1.001246        5814.0\n",
       "11  1.001245        6190.0\n",
       "4   1.001246        6235.0\n",
       "6   1.001247        4912.0\n",
       "19  1.001247        2760.0\n",
       "3   1.001247        3482.0\n",
       "12  1.001247        6801.0\n",
       "18  1.001247        2726.0\n",
       "7   1.001247        4864.0\n",
       "15  1.001247        3255.0\n",
       "14  1.001247        1811.0\n",
       "9   1.001247        1871.0\n",
       "17  1.001247        2282.0\n",
       "2   1.001247        2003.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_within_cluster_variance(data_df, cluster_to_articles):\n",
    "    scores = defaultdict(dict)\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Compute within-cluster variance for each cluster\n",
    "    for cluster_id, articles in tqdm(cluster_to_articles.items(), total=len(cluster_to_articles.items())):\n",
    "        filtered_articles = [article for article in articles if article in data_df.columns]\n",
    "        if len(filtered_articles) < 10:\n",
    "            continue  # Need at least 10 articles to compute any meaningful measure\n",
    "\n",
    "        # Extract and standardize data\n",
    "        category_data = scaler.fit_transform(data_df[filtered_articles].fillna(0).T)\n",
    "\n",
    "        # Use PCA to reduce dimensionality for large data sets\n",
    "        pca = PCA(n_components=min(category_data.shape) - 1 if category_data.shape[0] > 2 else 2)\n",
    "        reduced_data = pca.fit_transform(category_data)\n",
    "\n",
    "        # Calculate variance within the cluster\n",
    "        variance = np.var(reduced_data, axis=0).mean()  # Average variance across all components\n",
    "        scores[cluster_id] = [variance, len(filtered_articles)]\n",
    "\n",
    "    return pd.DataFrame(scores, index=['Variance', 'Num_Articles']).T\n",
    "\n",
    "within_cluster_variance = compute_within_cluster_variance(df, topic_to_articles)\n",
    "\n",
    "within_cluster_variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62e5dfcc-9d3d-4bf5-ae69-eafd822c8ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>13</th>\n",
       "      <th>16</th>\n",
       "      <th>1</th>\n",
       "      <th>8</th>\n",
       "      <th>0</th>\n",
       "      <th>10</th>\n",
       "      <th>5</th>\n",
       "      <th>11</th>\n",
       "      <th>4</th>\n",
       "      <th>6</th>\n",
       "      <th>19</th>\n",
       "      <th>3</th>\n",
       "      <th>12</th>\n",
       "      <th>18</th>\n",
       "      <th>7</th>\n",
       "      <th>15</th>\n",
       "      <th>14</th>\n",
       "      <th>9</th>\n",
       "      <th>17</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-07-01</th>\n",
       "      <td>10124971.0</td>\n",
       "      <td>29419089.0</td>\n",
       "      <td>42795832.0</td>\n",
       "      <td>64750778.0</td>\n",
       "      <td>9321821.0</td>\n",
       "      <td>24711855.0</td>\n",
       "      <td>26301627.0</td>\n",
       "      <td>33692641.0</td>\n",
       "      <td>32264321.0</td>\n",
       "      <td>14886638.0</td>\n",
       "      <td>11871212.0</td>\n",
       "      <td>9828925.0</td>\n",
       "      <td>15870210.0</td>\n",
       "      <td>9737856.0</td>\n",
       "      <td>24416955.0</td>\n",
       "      <td>18389903.0</td>\n",
       "      <td>6333682.0</td>\n",
       "      <td>6868109.0</td>\n",
       "      <td>5984525.0</td>\n",
       "      <td>8342568.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-02</th>\n",
       "      <td>10089112.0</td>\n",
       "      <td>28271119.0</td>\n",
       "      <td>41776052.0</td>\n",
       "      <td>67096147.0</td>\n",
       "      <td>9216624.0</td>\n",
       "      <td>23742452.0</td>\n",
       "      <td>25775156.0</td>\n",
       "      <td>32254407.0</td>\n",
       "      <td>30797235.0</td>\n",
       "      <td>14234917.0</td>\n",
       "      <td>11558745.0</td>\n",
       "      <td>9597687.0</td>\n",
       "      <td>15506870.0</td>\n",
       "      <td>9798491.0</td>\n",
       "      <td>22802824.0</td>\n",
       "      <td>18148746.0</td>\n",
       "      <td>6306469.0</td>\n",
       "      <td>6476890.0</td>\n",
       "      <td>5899968.0</td>\n",
       "      <td>8349222.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-03</th>\n",
       "      <td>9621115.0</td>\n",
       "      <td>27558367.0</td>\n",
       "      <td>39750851.0</td>\n",
       "      <td>63502550.0</td>\n",
       "      <td>8800607.0</td>\n",
       "      <td>22162277.0</td>\n",
       "      <td>25479714.0</td>\n",
       "      <td>30856697.0</td>\n",
       "      <td>30742547.0</td>\n",
       "      <td>13889369.0</td>\n",
       "      <td>11440798.0</td>\n",
       "      <td>9488362.0</td>\n",
       "      <td>14782261.0</td>\n",
       "      <td>9151174.0</td>\n",
       "      <td>22779877.0</td>\n",
       "      <td>17210956.0</td>\n",
       "      <td>6104186.0</td>\n",
       "      <td>6143090.0</td>\n",
       "      <td>5642849.0</td>\n",
       "      <td>7515333.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-04</th>\n",
       "      <td>10045626.0</td>\n",
       "      <td>30119400.0</td>\n",
       "      <td>45546694.0</td>\n",
       "      <td>67496135.0</td>\n",
       "      <td>9018124.0</td>\n",
       "      <td>24610430.0</td>\n",
       "      <td>25608157.0</td>\n",
       "      <td>31111275.0</td>\n",
       "      <td>33522981.0</td>\n",
       "      <td>14006628.0</td>\n",
       "      <td>11633518.0</td>\n",
       "      <td>11252645.0</td>\n",
       "      <td>16801252.0</td>\n",
       "      <td>9074687.0</td>\n",
       "      <td>23625741.0</td>\n",
       "      <td>17051395.0</td>\n",
       "      <td>6270223.0</td>\n",
       "      <td>6062413.0</td>\n",
       "      <td>5866762.0</td>\n",
       "      <td>8221497.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-05</th>\n",
       "      <td>10927807.0</td>\n",
       "      <td>32097612.0</td>\n",
       "      <td>45774538.0</td>\n",
       "      <td>70326200.0</td>\n",
       "      <td>9854583.0</td>\n",
       "      <td>25047360.0</td>\n",
       "      <td>27603064.0</td>\n",
       "      <td>34335325.0</td>\n",
       "      <td>37345057.0</td>\n",
       "      <td>15545599.0</td>\n",
       "      <td>12768789.0</td>\n",
       "      <td>10721934.0</td>\n",
       "      <td>16889162.0</td>\n",
       "      <td>10688184.0</td>\n",
       "      <td>26563391.0</td>\n",
       "      <td>19687148.0</td>\n",
       "      <td>6588101.0</td>\n",
       "      <td>6435513.0</td>\n",
       "      <td>6296921.0</td>\n",
       "      <td>8748261.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    13          16          1           8          0   \\\n",
       "2015-07-01  10124971.0  29419089.0  42795832.0  64750778.0  9321821.0   \n",
       "2015-07-02  10089112.0  28271119.0  41776052.0  67096147.0  9216624.0   \n",
       "2015-07-03   9621115.0  27558367.0  39750851.0  63502550.0  8800607.0   \n",
       "2015-07-04  10045626.0  30119400.0  45546694.0  67496135.0  9018124.0   \n",
       "2015-07-05  10927807.0  32097612.0  45774538.0  70326200.0  9854583.0   \n",
       "\n",
       "                    10          5           11          4           6   \\\n",
       "2015-07-01  24711855.0  26301627.0  33692641.0  32264321.0  14886638.0   \n",
       "2015-07-02  23742452.0  25775156.0  32254407.0  30797235.0  14234917.0   \n",
       "2015-07-03  22162277.0  25479714.0  30856697.0  30742547.0  13889369.0   \n",
       "2015-07-04  24610430.0  25608157.0  31111275.0  33522981.0  14006628.0   \n",
       "2015-07-05  25047360.0  27603064.0  34335325.0  37345057.0  15545599.0   \n",
       "\n",
       "                    19          3           12          18          7   \\\n",
       "2015-07-01  11871212.0   9828925.0  15870210.0   9737856.0  24416955.0   \n",
       "2015-07-02  11558745.0   9597687.0  15506870.0   9798491.0  22802824.0   \n",
       "2015-07-03  11440798.0   9488362.0  14782261.0   9151174.0  22779877.0   \n",
       "2015-07-04  11633518.0  11252645.0  16801252.0   9074687.0  23625741.0   \n",
       "2015-07-05  12768789.0  10721934.0  16889162.0  10688184.0  26563391.0   \n",
       "\n",
       "                    15         14         9          17         2   \n",
       "2015-07-01  18389903.0  6333682.0  6868109.0  5984525.0  8342568.0  \n",
       "2015-07-02  18148746.0  6306469.0  6476890.0  5899968.0  8349222.0  \n",
       "2015-07-03  17210956.0  6104186.0  6143090.0  5642849.0  7515333.0  \n",
       "2015-07-04  17051395.0  6270223.0  6062413.0  5866762.0  8221497.0  \n",
       "2015-07-05  19687148.0  6588101.0  6435513.0  6296921.0  8748261.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_aggregated_df(data_df, topic_to_articles):\n",
    "    # Initialize an empty DataFrame with the same index as data_df\n",
    "    aggregated_df = pd.DataFrame(index=data_df.index)\n",
    "\n",
    "    # Iterate over each topic and its articles\n",
    "    for topic_id, articles in topic_to_articles.items():\n",
    "        # Filter the articles that are present in the DataFrame\n",
    "        valid_articles = [article for article in articles if article in data_df.columns]\n",
    "        \n",
    "        if not valid_articles:\n",
    "            continue\n",
    "        \n",
    "        # Sum the view counts of the valid articles for each day\n",
    "        aggregated_df[topic_id] = data_df[valid_articles].sum(axis=1)\n",
    "    \n",
    "    return aggregated_df\n",
    "\n",
    "aggregated_df = create_aggregated_df(df, topic_to_articles)\n",
    "\n",
    "\n",
    "aggregated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "467e0e2f-e1c9-4963-84de-6b8148e58722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing SARIMA models for page: 13\n",
      "Best SARIMA model for 13: Order=(0, 0, 0) AIC=23413.63\n",
      "Processing SARIMA models for page: 16\n",
      "Best SARIMA model for 16: Order=(0, 1, 0) AIC=23606.59\n",
      "Processing SARIMA models for page: 1\n",
      "Best SARIMA model for 1: Order=(0, 1, 0) AIC=24083.14\n",
      "Processing SARIMA models for page: 8\n",
      "Best SARIMA model for 8: Order=(0, 1, 0) AIC=24858.81\n",
      "Processing SARIMA models for page: 0\n",
      "Best SARIMA model for 0: Order=(0, 0, 0) AIC=23675.14\n",
      "Processing SARIMA models for page: 10\n",
      "Best SARIMA model for 10: Order=(0, 1, 0) AIC=24007.15\n",
      "Processing SARIMA models for page: 5\n",
      "Best SARIMA model for 5: Order=(0, 0, 0) AIC=25021.78\n",
      "Processing SARIMA models for page: 11\n",
      "Best SARIMA model for 11: Order=(0, 1, 0) AIC=24629.90\n",
      "Processing SARIMA models for page: 4\n",
      "Best SARIMA model for 4: Order=(0, 1, 0) AIC=24362.28\n",
      "Processing SARIMA models for page: 6\n",
      "Best SARIMA model for 6: Order=(0, 1, 0) AIC=23084.16\n",
      "Processing SARIMA models for page: 19\n",
      "Best SARIMA model for 19: Order=(0, 1, 0) AIC=23387.61\n",
      "Processing SARIMA models for page: 3\n",
      "Best SARIMA model for 3: Order=(0, 0, 0) AIC=23692.45\n",
      "Processing SARIMA models for page: 12\n",
      "Best SARIMA model for 12: Order=(0, 1, 0) AIC=23448.91\n",
      "Processing SARIMA models for page: 18\n",
      "Best SARIMA model for 18: Order=(0, 1, 0) AIC=23250.24\n",
      "Processing SARIMA models for page: 7\n",
      "Best SARIMA model for 7: Order=(0, 1, 0) AIC=23655.96\n",
      "Processing SARIMA models for page: 15\n",
      "Best SARIMA model for 15: Order=(0, 1, 0) AIC=23073.06\n",
      "Processing SARIMA models for page: 14\n",
      "Best SARIMA model for 14: Order=(0, 0, 0) AIC=23337.58\n",
      "Processing SARIMA models for page: 9\n",
      "Best SARIMA model for 9: Order=(0, 1, 0) AIC=22112.64\n",
      "Processing SARIMA models for page: 17\n",
      "Best SARIMA model for 17: Order=(0, 1, 0) AIC=22272.03\n",
      "Processing SARIMA models for page: 2\n",
      "Best SARIMA model for 2: Order=(0, 0, 0) AIC=23014.15\n"
     ]
    }
   ],
   "source": [
    "def fit_sarima(series, p, d, q, P, D, Q, s, exog=None):\n",
    "    model = SARIMAX(series, exog=exog, order=(p, d, q), seasonal_order=(P, D, Q, s))\n",
    "    try:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "            fitted_model = model.fit(disp=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Fitting SARIMA({p},{d},{q}) x ({P},{D},{Q},{s}) failed with default start parameters: {e}\")\n",
    "        # Retry with initial parameters set to zero\n",
    "        try:\n",
    "            fitted_model = model.fit(start_params=[0] * (p + q + P + Q))\n",
    "        except Exception as e:\n",
    "            print(f\"Retry fitting SARIMA({p},{d},{q}) x ({P},{D},{Q},{s}) failed: {e}\")\n",
    "            return None\n",
    "    return fitted_model\n",
    "\n",
    "def find_best_sarima_model(time_series, p_values, d_values, q_values, P_values, D_values, Q_values, s, exog=None):\n",
    "    best_aic = np.inf\n",
    "    best_order = None\n",
    "    best_seasonal_order = None\n",
    "    best_model = None\n",
    "\n",
    "    for p in p_values:\n",
    "        for d in d_values:\n",
    "            for q in q_values:\n",
    "                for P in P_values:\n",
    "                    for D in D_values:\n",
    "                        for Q in Q_values:\n",
    "                            try:\n",
    "                                model = fit_sarima(time_series, p, d, q, P, D, Q, s, exog)\n",
    "                                if model is not None and model.aic < best_aic:\n",
    "                                    best_aic = model.aic\n",
    "                                    best_order = (p, d, q)\n",
    "                                    best_seasonal_order = (P, D, Q, s)\n",
    "                                    best_model = model\n",
    "                            except Exception as e:\n",
    "                                print(f\"Failed to fit SARIMA({p},{d},{q}) x ({P},{D},{Q},{s}): {str(e)}\")\n",
    "    \n",
    "    return {\"order\": best_order, \"seasonal_order\": best_seasonal_order, \"model\": best_model, \"aic\": best_aic}\n",
    "\n",
    "def get_time_series_and_exog(data, page_name):\n",
    "    time_series = data[page_name]\n",
    "    time_series = time_series.asfreq('D')\n",
    "    \n",
    "     # Day of the week\n",
    "    day_of_week = pd.to_datetime(time_series.index).dayofweek\n",
    "    exog_dow = pd.get_dummies(day_of_week, prefix='dow').astype(int)\n",
    "    exog_dow.index = time_series.index\n",
    "    \n",
    "    # Month of the year\n",
    "    month_of_year = pd.to_datetime(time_series.index).month\n",
    "    exog_month = pd.get_dummies(month_of_year, prefix='month').astype(int)\n",
    "    exog_month.index = time_series.index\n",
    "    \n",
    "    # Is weekend\n",
    "    is_weekend = (day_of_week >= 5).astype(int)\n",
    "    exog_weekend = pd.DataFrame(is_weekend, index=time_series.index, columns=['is_weekend'])\n",
    "    \n",
    "    # Is holiday\n",
    "    holidays = calendar().holidays(start=time_series.index.min(), end=time_series.index.max())\n",
    "    is_holiday = time_series.index.isin(holidays).astype(int)\n",
    "    exog_holiday = pd.DataFrame(is_holiday, index=time_series.index, columns=['is_holiday'])\n",
    "    \n",
    "    # Combine all exogenous features\n",
    "    exog = pd.concat([exog_dow, exog_month, exog_weekend, exog_holiday], axis=1)\n",
    "    \n",
    "    return time_series, exog\n",
    "\n",
    "def train_test_split(series, exog, test_size):\n",
    "    train = series[:-test_size]\n",
    "    test = series[-test_size:]\n",
    "    train_exog = exog[:-test_size]\n",
    "    test_exog = exog[-test_size:]\n",
    "    return train, test, train_exog, test_exog\n",
    "\n",
    "def process_page(page_name, data, date_columns, p_values, d_values, q_values, P_values, D_values, Q_values, S, test_size):\n",
    "    print(f\"Processing SARIMA models for page: {page_name}\")\n",
    "        \n",
    "    # Get the time series data for the page\n",
    "    time_series, exog = get_time_series_and_exog(data, page_name)\n",
    "    \n",
    "    # Skip the current iteration if no data was found\n",
    "    if time_series is None:\n",
    "        return\n",
    "\n",
    "    # Determine if the series is stationary\n",
    "    is_stationary = check_stationarity(time_series)\n",
    "\n",
    "    # Adjust d_values based on stationarity\n",
    "    adjusted_d_values = [0] if is_stationary else d_values\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train_series, test_series, train_exog, test_exog =  train_test_split(time_series, exog, test_size)\n",
    "\n",
    "    # Find the best ARIMA model for the time series\n",
    "    best_model_info = find_best_sarima_model(train_series, p_values, adjusted_d_values, q_values, P_values, D_values, Q_values, S, exog=train_exog)\n",
    "    \n",
    "    print(f\"Best SARIMA model for {page_name}: Order={best_model_info['order']} AIC={best_model_info['aic']:.2f}\")\n",
    "\n",
    "    return best_model_info\n",
    "\n",
    "\n",
    "def process_and_save_models(data, date_columns, p_values, d_values, q_values, P_values, D_values, Q_values, S, test_size):\n",
    "    # Iterate over all unique pages in the DataFrame\n",
    "    for page_name in data.columns:\n",
    "        best_model_info = process_page(page_name, data, date_columns, p_values, d_values, q_values, P_values, D_values, Q_values, S, test_size)\n",
    "        if best_model_info is not None:\n",
    "            # Save each model's information into a separate pickle file\n",
    "            with open(f'best_agg_model_{page_name}.pkl', 'wb') as f:\n",
    "                pickle.dump(best_model_info, f)\n",
    "\n",
    "p_values = [0]\n",
    "d_values = [1]\n",
    "q_values = [0]\n",
    "P_values = [1]\n",
    "D_values = [1]\n",
    "Q_values = [0]\n",
    "s = 60\n",
    "\n",
    "test_size = 30\n",
    "process_and_save_models(aggregated_df, date_columns, p_values, d_values, q_values, P_values, D_values, Q_values, s, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b5bad5-8618-45e8-b34b-bf2a9fe07429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bf2d367-1805-4ffb-ad39-2848da1ecd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def smape(actual, forecast):\n",
    "#     return 100 * np.mean(2 * np.abs(forecast - actual) / (np.abs(actual) + np.abs(forecast)))\n",
    "\n",
    "# def check_residuals(model):\n",
    "#     residuals = model.resid\n",
    "#     fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n",
    "#     residuals.plot(ax=ax[0], title=\"Residuals\")\n",
    "#     residuals.plot(kind='kde', ax=ax[1], title=\"Density\")\n",
    "#     plt.show()\n",
    "    \n",
    "# def plot_forecast_vs_actual(train_series, test_series, train_exog, test_exog, model):\n",
    "#     fig, ax = plt.subplots(figsize=(14, 7))\n",
    "#     ax.plot(train_series, label='Train', color='blue', linewidth=1)\n",
    "#     ax.plot(test_series, label='Test', color='orange', linewidth=1)\n",
    "    \n",
    "#     # In-sample forecast\n",
    "#     in_sample_forecast = model.fittedvalues\n",
    "#     ax.plot(in_sample_forecast, label='In-sample Forecast', color='green', linestyle='--', linewidth=2)\n",
    "    \n",
    "#     # Out-of-sample forecast\n",
    "#     forecast = model.get_forecast(steps=len(test_series), exog=test_exog)\n",
    "#     forecast_index = pd.date_range(start=test_series.index[0], periods=len(test_series), freq='D')\n",
    "#     forecast_series = pd.Series(forecast.predicted_mean, index=forecast_index)\n",
    "#     ax.plot(forecast_series, label='Out-of-sample Forecast', color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "#     sMAPE_value = smape(test_series, forecast_series)\n",
    "#     print(f'sMAPE: {sMAPE_value:.2f}%')\n",
    "    \n",
    "#     # Adding titles and labels\n",
    "#     ax.set_title('Actual vs Forecasted Values', fontsize=16)\n",
    "#     ax.set_xlabel('Date', fontsize=14)\n",
    "#     ax.set_ylabel('Page Views', fontsize=14)\n",
    "    \n",
    "#     # Adding legend\n",
    "#     ax.legend(loc='upper left', fontsize=12)\n",
    "    \n",
    "#     # Adding grid for better readability\n",
    "#     ax.grid(True)\n",
    "    \n",
    "#     # Improving the appearance\n",
    "#     plt.tight_layout()\n",
    "    \n",
    "#     plt.show()\n",
    "\n",
    "# def plot_model_forecast(data, best_models_per_page, date_columns, page_name, test_size):\n",
    "#     if page_name not in best_models_per_page:\n",
    "#         print(f\"No model found for page: {page_name}\")\n",
    "#         return\n",
    "    \n",
    "#     best_model_info = best_models_per_page[page_name]\n",
    "#     model = best_model_info['model']\n",
    "    \n",
    "#     time_series, exog = get_time_series_and_exog(data, page_name)\n",
    "    \n",
    "#     if time_series is not None:\n",
    "#         train_series, test_series, train_exog, test_exog = train_test_split(time_series, exog, test_size)\n",
    "#         plot_forecast_vs_actual(train_series, test_series, train_exog, test_exog, model)\n",
    "#         check_residuals(model)\n",
    "\n",
    "\n",
    "# page_name = 0\n",
    "\n",
    "# plot_model_forecast(aggregated_df, best_models_per_page, date_columns, page_name, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8872d547-69ca-45ab-ba1a-f2cc7ed1c0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming aggregated_df is your DataFrame with dates as the index\n",
    "aggregated_df.to_csv('aggregated_df.csv', index_label='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e4a26d9-47a5-44af-b130-0a08cbf28b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('best_models_per_page.pkl', 'wb') as f:\n",
    "#     pickle.dump(best_models_per_page, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98c774e2-61a9-4b9c-9f8a-7a97608cdbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# best_models_per_page_str_keys = {str(key): value for key, value in best_models_per_page.items()}\n",
    "\n",
    "# # Serialize and save the dictionary to a file\n",
    "# with open('best_models_per_page.pkl', 'wb') as f:\n",
    "#     pickle.dump(best_models_per_page_str_keys, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0569ea2-ab00-4b32-96da-012403d9b505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af76d74-e031-495c-8879-62de2fc3e0d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c810502a-d35d-4efb-99b5-bb6fd93655d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22709174-9c94-40c0-b032-20926d6a93aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebe7891-038d-4ad7-9f81-851eceff73e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3482ab7-df57-47d1-beb1-48ca13569d58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3504f5d-6465-47b5-9b4f-f6091222a475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da9d30e-413a-413d-abf8-b6c586bdbdd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489b6cd3-122b-4e88-8913-d83616d8cd8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cf2851-27f8-4a13-b563-99c895b79747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2baa342-dbde-40dd-baef-62e046a99177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55338e9e-15ea-4059-9a54-465345a81bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166d6b84-044e-41cb-872e-f7f426f5e639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7289882-c71b-49ab-bedd-582f77cd203b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fb6451-dfb6-4c19-bc8a-3681ff45fafa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3712593c-f7e7-488c-84e7-93343bb73a47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fbf7c7-79aa-4c62-ac0a-691f7ee21386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596ab028-b12a-4eab-91d9-6b8ab3decca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b85cad-19e7-42c5-aac7-b237b4d18e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534d0bca-7aef-43fb-8897-39027226d70f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8c1bd1-74c2-4481-bc35-713f974414de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "793f13c6-faa6-4a06-b448-c4ecca6a56ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_names = {\n",
    "#     0: \"Biographical Information and Entertainment\",\n",
    "#     1: \"Articles Needing References and Identifiers\",\n",
    "#     2: \"Television Series and Science Fiction\",\n",
    "#     3: \"Films and Literature\",\n",
    "#     4: \"African American and European Establishments\",\n",
    "#     5: \"Geographical Locations and Establishments\",\n",
    "#     6: \"Deaths, Actors, and Sports\",\n",
    "#     7: \"External Links and History\",\n",
    "#     8: \"Geography and Sports Stubs\",\n",
    "#     9: \"Writers and Politicians\",\n",
    "#     10: \"Articles Containing Text and Statements\",\n",
    "#     11: \"Albums and University Establishments\",\n",
    "#     12: \"Songs and Sports Events\",\n",
    "#     13: \"Fictional Characters and Competitions\",\n",
    "#     14: \"Players and Structures\",\n",
    "#     15: \"People and Football\",\n",
    "#     16: \"Software and Political Information\",\n",
    "#     17: \"Redirects and Films\",\n",
    "#     18: \"People of Various Descent\",\n",
    "#     19: \"United States and Businesspeople\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf07b3a-6d2c-4146-82f1-fbb2be694101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca984cd-b268-478e-b19e-2e1961c8c45b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9f4dfdec-b9a1-49fd-9cb4-dbb88ce8a3fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceafe4c-4e4a-498f-9bfd-23820289b0ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9634179e-d2bc-48d8-877c-f7f9c5791d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_pca_for_two_topics(views_df, topic_to_articles, topic1, topic2, num_days=10):\n",
    "#     # Select articles from the two topics\n",
    "#     articles_topic1 = set(topic_to_articles[topic1])\n",
    "#     articles_topic2 = set(topic_to_articles[topic2])\n",
    "\n",
    "#     # Identify common articles and remove them from one of the topics\n",
    "#     common_articles = articles_topic1.intersection(articles_topic2)\n",
    "#     articles_topic1 = articles_topic1 - common_articles\n",
    "#     articles_topic2 = articles_topic2 - common_articles\n",
    "\n",
    "#     # Combine the remaining articles\n",
    "#     selected_articles = list(articles_topic1.union(articles_topic2))\n",
    "    \n",
    "#     view_counts = views_df[selected_articles].iloc[:num_days].fillna(0).T\n",
    "\n",
    "#     # Standardize the data\n",
    "#     scaler = StandardScaler()\n",
    "#     view_counts_scaled = scaler.fit_transform(view_counts)\n",
    "\n",
    "#     # Apply PCA\n",
    "#     pca = PCA(n_components=2)\n",
    "#     principal_components = pca.fit_transform(view_counts_scaled)\n",
    "\n",
    "#     # Create a DataFrame with the principal components\n",
    "#     pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "#     pca_df['Topic'] = ['Topic1'] * len(articles_topic1) + ['Topic2'] * len(articles_topic2)\n",
    "\n",
    "#     # Plot the results\n",
    "#     plt.figure(figsize=(10, 7))\n",
    "#     colors = {'Topic1': 'red', 'Topic2': 'blue'}\n",
    "#     for topic in ['Topic1', 'Topic2']:\n",
    "#         indices_to_keep = pca_df['Topic'] == topic\n",
    "#         plt.scatter(pca_df.loc[indices_to_keep, 'PC1'],\n",
    "#                     pca_df.loc[indices_to_keep, 'PC2'],\n",
    "#                     c=colors[topic], s=50)\n",
    "\n",
    "#     plt.title(f'PCA of Articles in Topics {topic1} and {topic2}')\n",
    "#     plt.xlabel('Principal Component 1')\n",
    "#     plt.ylabel('Principal Component 2')\n",
    "#     plt.legend(['Topic1', 'Topic2'])\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# plot_pca_for_two_topics(df, topic_to_articles, topic1=0, topic2=1, num_days=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd50a6a-a180-4827-9348-292c856b0ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0705464a-b4e8-4c1b-a419-bfb97322826e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dff519-fa6d-43a8-9c2c-82ec48b07c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830a4418-4e0c-4bf9-a550-dd0302bc3f48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7058fd35-8942-4eb6-ae2b-7c0db11e64d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:13<00:00,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Software and Political Information              1.001247\n",
      "Biographical Information and Entertainment      1.001247\n",
      "Films and Literature                            1.001246\n",
      "External Links and History                      1.001246\n",
      "Articles Containing Text and Statements         1.001246\n",
      "Articles Needing References and Identifiers     1.001246\n",
      "Redirects and Films                             1.001245\n",
      "United States and Businesspeople                1.001246\n",
      "People and Football                             1.001247\n",
      "Albums and University Establishments            1.001246\n",
      "Geographical Locations and Establishments       1.001247\n",
      "Players and Structures                          1.001247\n",
      "Songs and Sports Events                         1.001247\n",
      "People of Various Descent                       1.001246\n",
      "Geography and Sports Stubs                      1.001247\n",
      "Writers and Politicians                         1.001246\n",
      "Television Series and Science Fiction           1.001247\n",
      "Deaths, Actors, and Sports                      1.001247\n",
      "African American and European Establishments    1.001247\n",
      "Fictional Characters and Competitions           1.001247\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_within_cluster_variance(data_df, category_to_topic, topic_names):\n",
    "    scores = defaultdict(dict)\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Group articles by topics\n",
    "    topic_to_articles = defaultdict(list)\n",
    "    for category, topic in category_to_topic.items():\n",
    "        topic_to_articles[topic].extend(cat_to_articles[category])\n",
    "\n",
    "    # Compute within-cluster variance for each topic\n",
    "    for topic, articles in tqdm(topic_to_articles.items(), total=len(topic_to_articles.items())):\n",
    "        filtered_articles = [article for article in articles if article in data_df.columns]\n",
    "        if len(filtered_articles) < 10:\n",
    "            continue  # Need at least 10 articles to compute any meaningful measure\n",
    "\n",
    "        # Extract and standardize data\n",
    "        category_data = scaler.fit_transform(data_df[filtered_articles].fillna(0).T)\n",
    "\n",
    "        # Use PCA to reduce dimensionality for large data sets\n",
    "        pca = PCA(n_components=min(category_data.shape) - 1 if category_data.shape[0] > 2 else 2)\n",
    "        reduced_data = pca.fit_transform(category_data)\n",
    "\n",
    "        # Calculate variance within the cluster\n",
    "        variance = np.var(reduced_data, axis=0).mean()  # Average variance across all components\n",
    "        scores[topic_names[topic]] = variance\n",
    "\n",
    "    return pd.Series(scores)\n",
    "\n",
    "within_cluster_variance = compute_within_cluster_variance(df, category_to_topic, topic_names)\n",
    "\n",
    "print(within_cluster_variance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3752b98f-4690-4e01-ad50-7457f9079fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4minute',\n",
       " 'ASTRO',\n",
       " 'BEAST',\n",
       " 'BLACK_PINK',\n",
       " 'BLEACH',\n",
       " 'BTOB',\n",
       " 'CHAOS;HEAD',\n",
       " 'DIA',\n",
       " 'Doctors',\n",
       " 'EGOIST',\n",
       " 'EXO',\n",
       " 'FAIRY_TAIL',\n",
       " 'IKON',\n",
       " 'MAMAMOO',\n",
       " 'NEW_GAME!',\n",
       " 'ONE_OK_ROCK',\n",
       " 'ONE_PIECE',\n",
       " 'RADWIMPS',\n",
       " 'Red_Velvet',\n",
       " 'SHINee',\n",
       " 'SISTAR',\n",
       " 'Schwarzesmarken',\n",
       " 'TWICE',\n",
       " 'Twins',\n",
       " 'Xss',\n",
       " 'xss',\n",
       " '日本',\n",
       " '張惠妹',\n",
       " '嵐',\n",
       " '0.999…',\n",
       " '曾鈺成',\n",
       " '粽',\n",
       " '中國',\n",
       " '大熊猫',\n",
       " '@',\n",
       " 'Amazon.com',\n",
       " 'Art_nouveau',\n",
       " 'Australie',\n",
       " \"Ballon_d'or\",\n",
       " 'Barcelone',\n",
       " 'Basket-ball',\n",
       " 'Belgique',\n",
       " 'Blade_Runner_(film)',\n",
       " 'Blindspot',\n",
       " 'Blue_Bloods',\n",
       " 'Californie',\n",
       " 'Designated_Survivor',\n",
       " 'Doctor_Strange_(film)',\n",
       " 'Donald_Trump,_Jr.',\n",
       " 'Environnement',\n",
       " 'Fences',\n",
       " 'Finlande',\n",
       " 'Game_of_thrones',\n",
       " 'Gengis_Khan',\n",
       " 'Gravity_(film)',\n",
       " 'Groenland',\n",
       " 'Int',\n",
       " 'JoeyStarr',\n",
       " 'Kim_Jong-un',\n",
       " 'Labrador_retriever',\n",
       " 'Le_Clan_des_Siciliens',\n",
       " 'Marie_Stuart',\n",
       " 'Martin_Luther_King',\n",
       " 'Nikola_Karabatic',\n",
       " 'Porto_Rico',\n",
       " 'Punisher_(comics)',\n",
       " 'Serbie',\n",
       " 'Singapour',\n",
       " 'Sp',\n",
       " 'Split_(film)',\n",
       " 'Steve_Holmes',\n",
       " 'Tess_(film)',\n",
       " 'The_Leftovers',\n",
       " 'The_Magicians',\n",
       " 'True_blood',\n",
       " 'Vladimir_Poutine',\n",
       " 'XXX_(film)',\n",
       " 'Zouaves',\n",
       " 'A_Silent_Voice',\n",
       " 'Affiche_rouge',\n",
       " 'Argentine',\n",
       " 'Avengers_(film)',\n",
       " 'Blackstar',\n",
       " 'Bones',\n",
       " 'Dunkirk_(film)',\n",
       " 'Eagles',\n",
       " 'La_La_Land_(film)',\n",
       " 'M.I.A.',\n",
       " 'Maine_coon',\n",
       " 'Major_Crimes',\n",
       " 'Mardi_gras',\n",
       " 'Mohamed_Ali',\n",
       " 'Olympique_lyonnais',\n",
       " 'PEGIDA',\n",
       " \"Palme_d'or\",\n",
       " 'Passengers_(film)',\n",
       " 'Prometheus_(film)',\n",
       " 'Russie',\n",
       " 'Suicide_Squad_(film)',\n",
       " 'The_Expanse',\n",
       " 'Céline_Dion',\n",
       " 'Fête_de_la_musique',\n",
       " 'Noël',\n",
       " 'La_Bonne_Année',\n",
       " 'Monténégro',\n",
       " 'Léonard_de_Vinci',\n",
       " 'Académie_française',\n",
       " 'Arménie',\n",
       " 'Israël',\n",
       " 'Laïcité',\n",
       " 'Québec',\n",
       " 'Sénégal',\n",
       " 'La_Planète_des_singes',\n",
       " 'Écosse',\n",
       " 'Cinéma',\n",
       " \"Ordre_national_de_la_Légion_d'honneur\",\n",
       " 'Wikipédia_en_français',\n",
       " 'Éric_Cantona',\n",
       " 'République_dominicaine',\n",
       " 'Nouvelle-Calédonie',\n",
       " 'Indonésie',\n",
       " 'Planète',\n",
       " 'La_Réunion',\n",
       " 'Loïe_Fuller',\n",
       " 'Suède',\n",
       " 'Moïse',\n",
       " 'Pérou',\n",
       " 'Montréal',\n",
       " 'Les_bronzés_font_du_ski',\n",
       " 'Pokémon_GO',\n",
       " '1989_(Taylor_Swift_album)',\n",
       " '2016_NFL_Draft',\n",
       " '2016_NHL_Entry_Draft',\n",
       " 'Agent_Carter_(season_2)',\n",
       " 'Agents_of_S.H.I.E.L.D._(season_4)',\n",
       " 'Alex_Jones_(radio_host)',\n",
       " 'Alice_Through_the_Looking_Glass_(film)',\n",
       " \"America's_Got_Talent_(season_11)\",\n",
       " 'American_Idol_(season_15)',\n",
       " 'Anonymous_(group)',\n",
       " 'Ashley_Graham_(model)',\n",
       " \"Auli'i_Cravalho\",\n",
       " 'BTS_(band)',\n",
       " 'Battlefield_(series)',\n",
       " 'Blockchain_(database)',\n",
       " 'Blue_&_Lonesome_(The_Rolling_Stones_album)',\n",
       " 'Bowling_Green_Massacre',\n",
       " 'Cities_and_towns_during_the_Syrian_Civil_War',\n",
       " 'Dark_Matter_(TV_series)',\n",
       " 'David_bowie',\n",
       " 'Dualism_(philosophy_of_mind)',\n",
       " 'Eddie_\"The_Eagle\"_Edwards',\n",
       " 'Elizabeth_I_of_England',\n",
       " 'Exo_(band)',\n",
       " 'Fibonacci_number',\n",
       " 'Fucking,_Austria',\n",
       " 'Glitch_(TV_series)',\n",
       " 'God_of_War_(series)',\n",
       " 'Golden_Globe_Award',\n",
       " 'Historical_rankings_of_Presidents_of_the_United_States',\n",
       " 'Ida_Lewis_(lighthouse_keeper)',\n",
       " 'Internet_of_Things',\n",
       " 'Jay_Z',\n",
       " 'John_F._Kelly_(Marine)',\n",
       " 'John_Lewis_(Georgia_politician)',\n",
       " 'List_of_The_Flash_(2014_TV_series)_episodes',\n",
       " 'List_of_films_based_on_Marvel_Comics',\n",
       " 'List_of_most_disliked_YouTube_videos',\n",
       " 'List_of_most_viewed_YouTube_videos',\n",
       " 'Main_Page/',\n",
       " 'Matt_Smith_(actor)',\n",
       " 'Pizzagate_(conspiracy_theory)',\n",
       " 'Rangoon_(2017_film)',\n",
       " 'Running_Man_(TV_series)',\n",
       " 'Sing_(2016_film)',\n",
       " 'Sophie_Turner_(actress)',\n",
       " 'Star_Wars_Episode_II:_Attack_of_the_Clones',\n",
       " 'The_5th_Wave_(novel)',\n",
       " 'The_Championships,_Wimbledon',\n",
       " 'The_Visit_(2015_film)',\n",
       " 'Victoria_(TV_series)',\n",
       " 'Zealandia_(continent)',\n",
       " 'Zoe_Saldana',\n",
       " 'Agents_of_S.H.I.E.L.D._(season_3)',\n",
       " 'Ajax_(comics)',\n",
       " 'Archer_(TV_series)',\n",
       " 'Barack_obama',\n",
       " 'Bielefeld_Conspiracy',\n",
       " 'Deanne_pandey',\n",
       " 'Drake_(rapper)',\n",
       " 'Fan_(film)',\n",
       " 'George_III_of_the_United_Kingdom',\n",
       " 'Henry_VIII_of_England',\n",
       " 'Labyrinth_(film)',\n",
       " 'Mission_Hill',\n",
       " 'Mr._Robot_(TV_series)',\n",
       " 'NCT_(band)',\n",
       " 'Patrick_Wilson_(American_actor)',\n",
       " 'Raees_(film)',\n",
       " 'Star_Trek_(film)',\n",
       " 'Star_Wars_Episode_III:_Revenge_of_the_Sith',\n",
       " 'Suits_(TV_series)',\n",
       " 'The_Blacklist_(TV_series)',\n",
       " 'The_Dark_Knight_(film)',\n",
       " 'The_Founder_(film)',\n",
       " 'The_Night_Manager_(miniseries)',\n",
       " 'The_Office_(U.S._TV_series)',\n",
       " 'This_Is_Us_(TV_series)',\n",
       " 'Veronica_Porsche_Ali',\n",
       " 'Daredevil_(Marvel_Comics)',\n",
       " 'Diamondback_(comics)',\n",
       " 'Doom_(series)',\n",
       " 'Eddie_Fisher_(singer)',\n",
       " 'Ghost_Rider_(comics)',\n",
       " 'Grammy_Award',\n",
       " 'Marco_Polo_(TV_series)',\n",
       " 'Supergirl_(U.S._TV_series)',\n",
       " 'Syrian_Civil_War',\n",
       " 'The_Apprentice_(U.S._TV_series)',\n",
       " 'The_Man_Who_Knew_Infinity_(film)',\n",
       " 'The_Missing_(TV_series)',\n",
       " 'Uber_(company)',\n",
       " 'We_Happy_Few_(video_game)',\n",
       " \"Women's_March_on_Washington\",\n",
       " 'Armenian_Genocide',\n",
       " \"Bridget_Jones's_Diary_(film)\",\n",
       " 'List_of_Presidents_of_the_United_States',\n",
       " 'Madonna_(entertainer)',\n",
       " 'Saúl_Álvarez',\n",
       " \"Bahá'í_Faith\",\n",
       " 'Pokémon_(anime)',\n",
       " 'Battle_of_Aleppo_(2012–present)',\n",
       " \"2016_French_Open_–_Women's_Singles\",\n",
       " \"2016_Australian_Open_–_Women's_Singles\",\n",
       " \"2016_Australian_Open_–_Men's_Singles\",\n",
       " \"2016_French_Open_–_Men's_Singles\",\n",
       " 'Tennis_at_the_2016_Summer_Olympics_–_Mixed_Doubles',\n",
       " \"2016_Wimbledon_Championships_–_Men's_Singles\",\n",
       " \"2016_Wimbledon_Championships_–_Women's_Singles\",\n",
       " \"2016_US_Open_–_Women's_Singles\",\n",
       " \"2016_US_Open_–_Men's_Singles\",\n",
       " \"2017_Australian_Open_–_Women's_Singles\",\n",
       " \"2017_Australian_Open_–_Men's_Singles\",\n",
       " 'Italia',\n",
       " 'Main_page',\n",
       " 'España',\n",
       " 'St._Vincent',\n",
       " 'Википедия',\n",
       " 'Мальта',\n",
       " 'Аляска',\n",
       " 'L10n',\n",
       " 'Mediawiki',\n",
       " 'Namespaces',\n",
       " 'Quality_Assurance',\n",
       " 'Developers',\n",
       " 'Index.php',\n",
       " 'Localisation',\n",
       " 'Mailing_lists',\n",
       " 'Maps',\n",
       " 'Texvc',\n",
       " 'Wikimedia',\n",
       " 'mediawiki',\n",
       " 'Events',\n",
       " 'Meetings',\n",
       " 'Templates',\n",
       " 'I18n',\n",
       " 'Notifications',\n",
       " 'Abderrazak_Hamed_Allah',\n",
       " 'Barbarians',\n",
       " 'Canonisation',\n",
       " 'Chefs',\n",
       " 'Dita_von_Teese',\n",
       " 'Erik_Orsenna',\n",
       " 'Florence_Griffith-Joyner',\n",
       " 'Le_Vieux_Fusil',\n",
       " 'Loving_(film)',\n",
       " 'Reykjavik',\n",
       " 'Saint-nectaire',\n",
       " 'Salt_(film)',\n",
       " 'Seven_(film)',\n",
       " 'The_Duchess',\n",
       " 'Divines',\n",
       " 'Earvin_Ngapeth',\n",
       " 'Mohammed_Ali',\n",
       " 'Samis',\n",
       " \"Coup_d'État\",\n",
       " 'Crédit_agricole',\n",
       " 'Bénin',\n",
       " 'Alexander_Zverev_Jr.',\n",
       " 'American_Horror_Story:_My_Roanoke_Nightmare',\n",
       " 'Anthony_Davis_(basketball)',\n",
       " 'Cheryl_(entertainer)',\n",
       " 'George_IV_of_the_United_Kingdom',\n",
       " 'John_Scott_(ice_hockey)',\n",
       " 'Magneto_(comics)',\n",
       " 'Rooting_(Android_OS)',\n",
       " 'Dileep_(actor)',\n",
       " \"I'm_a_Celebrity...Get_Me_Out_of_Here!_(UK_TV_series)\",\n",
       " 'Luke_Evans_(actor)',\n",
       " 'Machine_Gun_Kelly_(rapper)',\n",
       " 'No_Country_for_Old_Men_(film)',\n",
       " 'Cottonmouth_(comics)',\n",
       " 'Love_Yourself_(Justin_Bieber_song)',\n",
       " 'Tom_Williams_(footballer)',\n",
       " 'Amphetamin',\n",
       " 'Argentinien',\n",
       " 'Australien',\n",
       " 'Beachvolleyball',\n",
       " 'Biene_Maja',\n",
       " 'Chief_Executive_Officer',\n",
       " 'Chris_Evans',\n",
       " 'Dr._House',\n",
       " 'Electoral_College',\n",
       " 'Finnland',\n",
       " 'Grand_Slam_(Tennis)',\n",
       " 'Herpes_Zoster',\n",
       " 'Humans',\n",
       " 'Independence_Day_(1996)',\n",
       " 'Italien',\n",
       " 'Jupiter_(Planet)',\n",
       " 'Leicester_City',\n",
       " 'Madagaskar',\n",
       " 'Marokko',\n",
       " 'Mispel',\n",
       " 'Mittelmeer',\n",
       " 'Niqab',\n",
       " 'Passengers_(2016)',\n",
       " 'Penny_Dreadful',\n",
       " 'Queen_(Band)',\n",
       " 'Real_Madrid',\n",
       " 'Shades_of_Blue',\n",
       " 'Star-Crossed',\n",
       " 'Taharrush_gamea',\n",
       " 'Thor_(Film)',\n",
       " 'Titanic_(1997)',\n",
       " 'Tom_Ellis',\n",
       " 'Usbekistan',\n",
       " 'Virtual_Private_Network',\n",
       " 'American_Football',\n",
       " 'Battleship_(Film)',\n",
       " 'Flixbus',\n",
       " 'Gambia',\n",
       " 'Golden_Goal',\n",
       " 'Grand_Budapest_Hotel',\n",
       " 'Irland',\n",
       " 'Rewe_Group',\n",
       " 'Swing_State',\n",
       " 'Ash_vs._Evil_Dead',\n",
       " 'Batterie',\n",
       " 'Black_Swan',\n",
       " 'Mexiko',\n",
       " 'Spezial:Beobachtungsliste',\n",
       " 'The_Middle',\n",
       " 'Roman_Polański',\n",
       " 'Jim_Knopf_und_Lukas_der_Lokomotivführer',\n",
       " 'Ana_Ivanović',\n",
       " 'Serdar_Taşçı',\n",
       " 'Kaiser-Wilhelm-Gedächtniskirche',\n",
       " 'Vereinigtes_Königreich',\n",
       " 'Carl_Friedrich_Gauß',\n",
       " 'Kölner_Dom',\n",
       " 'Grey’s_Anatomy',\n",
       " 'Pokémon_(Anime)',\n",
       " 'Marvel’s_Agents_of_S.H.I.E.L.D.',\n",
       " 'Dalida_(film)',\n",
       " 'Imitation_Game',\n",
       " 'Marguerite_(film)',\n",
       " 'Roms',\n",
       " 'Captain_America:_First_Avenger',\n",
       " 'NRJ_Music_Awards',\n",
       " 'Nikki_Deloach',\n",
       " 'Top_Gun_(film)',\n",
       " 'Étoile_filante',\n",
       " 'ASIAN_KUNG-FU_GENERATION',\n",
       " 'BLACK_LAGOON',\n",
       " 'BTOOOM!',\n",
       " 'DAIGO',\n",
       " 'GANTZ',\n",
       " 'ICONIQ',\n",
       " 'LUNA_SEA',\n",
       " 'MAJOR',\n",
       " 'NANA',\n",
       " \"OKAMOTO'S\",\n",
       " 'ORANGE_RANGE',\n",
       " 'PKCZ',\n",
       " 'PUFFY',\n",
       " 'Pokemon_GO',\n",
       " 'Psycho_le_Cemu',\n",
       " 'STEINS;GATE',\n",
       " 'SUGIZO',\n",
       " 'THE_YELLOW_MONKEY',\n",
       " 'TM_NETWORK',\n",
       " 'UVERworld',\n",
       " 'YOSHIKI',\n",
       " 'ASKA',\n",
       " 'BABYMETAL',\n",
       " 'NON_STYLE',\n",
       " 'SOFT_BALLET',\n",
       " 'BOYS_AND_MEN',\n",
       " 'CHAGE_and_ASKA',\n",
       " 'DREAMS_COME_TRUE',\n",
       " 'LiSA',\n",
       " 'PATA',\n",
       " 'ZARD',\n",
       " 'GReeeeN',\n",
       " 'MONSTER',\n",
       " 'MUTEKI',\n",
       " 'CHARA',\n",
       " 'TOKIO',\n",
       " 'X_JAPAN',\n",
       " 'KARA',\n",
       " 'SASUKE',\n",
       " 'BOOM_BOOM_SATELLITES',\n",
       " 'BUMP_OF_CHICKEN',\n",
       " 'DEAN_FUJIOKA',\n",
       " 'あ',\n",
       " '鈴木貫太郎',\n",
       " '川崎宗則',\n",
       " '島崎信長',\n",
       " '17._Mai',\n",
       " 'Aborigines',\n",
       " 'Aristoteles',\n",
       " 'Capybaras',\n",
       " 'Content_Delivery_Network',\n",
       " 'Deep_Web',\n",
       " 'E-Mail',\n",
       " 'Gorillas',\n",
       " 'Islom_Karimov',\n",
       " 'Manchester_United',\n",
       " 'Near_Field_Communication',\n",
       " 'Yi_qi',\n",
       " 'Straßburg',\n",
       " 'Sinéad_O’Connor',\n",
       " 'Aluminio',\n",
       " 'Apolo_11',\n",
       " 'Arpa',\n",
       " 'Eurocopa',\n",
       " 'Inca',\n",
       " 'Irlanda',\n",
       " 'Mitocondria',\n",
       " 'Robert_Downey,_Jr.',\n",
       " 'Rumania',\n",
       " 'The_Martian',\n",
       " 'Unesco',\n",
       " 'Nuevo_Leon',\n",
       " '16_de_septiembre',\n",
       " 'República_Dominicana',\n",
       " 'Fútbol',\n",
       " 'Montaña',\n",
       " 'Moisés',\n",
       " 'Perú',\n",
       " 'Ciudad_de_México',\n",
       " 'Río',\n",
       " 'Átomo',\n",
       " 'Canadá',\n",
       " 'Día_de_la_Raza',\n",
       " 'Napoleón_Bonaparte',\n",
       " 'René_Laënnec',\n",
       " 'Irán',\n",
       " 'México',\n",
       " 'Wikipedia_en_español',\n",
       " 'Línea',\n",
       " 'Martín_de_Porres',\n",
       " 'Revolución_de_Mayo',\n",
       " 'Chichén_Itzá',\n",
       " 'Academy_Award',\n",
       " 'Beaches_(film)',\n",
       " 'Helen_Glover_(rower)',\n",
       " 'Kingpin_(comics)',\n",
       " 'List_of_Disney_Channel_Original_Movies',\n",
       " 'Matt_Baker_(presenter)',\n",
       " 'The_Bachelor_(U.S._TV_series)',\n",
       " 'William_IV_of_the_United_Kingdom',\n",
       " 'Black_death',\n",
       " 'Naagin_(TV_series)',\n",
       " 'Tom_Herman_(American_football)',\n",
       " 'List_of_Chief_Ministers_of_Tamil_Nadu',\n",
       " 'AKBINGO!',\n",
       " 'GLAY',\n",
       " 'HELLSING',\n",
       " 'SOULHEAD',\n",
       " 'SPEED',\n",
       " 'VC-25',\n",
       " 'Yahoo!_JAPAN',\n",
       " 'ZABADAK',\n",
       " '酉',\n",
       " '中国',\n",
       " '月',\n",
       " 'Fantôme',\n",
       " 'Croacia',\n",
       " 'Descendants',\n",
       " 'Dislexia',\n",
       " 'Malcolm_in_the_middle',\n",
       " 'Rings',\n",
       " 'Suits',\n",
       " 'The_Boy',\n",
       " 'Edgar_Perea',\n",
       " 'El_internado',\n",
       " 'Jamsa',\n",
       " 'Evita_Muñoz_\"Chachita\"',\n",
       " 'República',\n",
       " 'Julio_Iglesias,_Jr.',\n",
       " 'La_hija_del_mariachi',\n",
       " 'Sigla',\n",
       " 'La_reina_del_sur',\n",
       " 'The_Blind_Side',\n",
       " 'Concacaf',\n",
       " 'Lazos_de_amor',\n",
       " 'Ámsterdam',\n",
       " 'Niña_amada_mía',\n",
       " 'Pasión_de_gavilanes',\n",
       " 'Scorpions',\n",
       " 'Life_is_Strange',\n",
       " 'UNO',\n",
       " '23_(song)',\n",
       " \"Al-Qa'im_bi-Amr_Allah\",\n",
       " 'Ban_Mo_District',\n",
       " 'Betrayal_(book)',\n",
       " 'Cair_Paravel-Latin_School',\n",
       " 'Dance_Me_to_the_End_of_Love_(song)',\n",
       " 'Disney_Infinity_(series)',\n",
       " 'Edward_Short_(Canadian_judge)',\n",
       " 'Eric_Wilson_(American_football)',\n",
       " 'Glas_(disambiguation)',\n",
       " 'Gonadosomatic_Index',\n",
       " 'Ian_Turner_(footballer)',\n",
       " 'Irus_(genus)',\n",
       " 'Jack_of_clubs_(disambiguation)',\n",
       " 'John_Mitchell_(baseball)',\n",
       " 'Judith_(play)',\n",
       " 'NOP_(disambiguation)',\n",
       " 'Om_Prakash_Sharma_(politician)',\n",
       " 'Over,_Gloucestershire',\n",
       " 'People_Power_(disambiguation)',\n",
       " 'Poppin',\n",
       " 'Questa_Notte',\n",
       " 'Rupture_(film)',\n",
       " 'Say_(song)',\n",
       " 'Slide_Lake_(disambiguation)',\n",
       " 'Steers_(disambiguation)',\n",
       " 'Stoats_in_New_Zealand',\n",
       " 'Sungai_Tinggi_(state_constituency)',\n",
       " 'The_Blood_Line',\n",
       " 'The_Fury_(novel)',\n",
       " 'Thicker_Than_Water_(TV_series)',\n",
       " 'Time_and_Chance_(novel)',\n",
       " 'Tisiphone_(genus)',\n",
       " 'Yousef_Erakat',\n",
       " 'Zavet_(disambiguation)',\n",
       " 'Achilleus',\n",
       " 'Grease_(Film)',\n",
       " 'Hattrick',\n",
       " 'Martini_(Cocktail)',\n",
       " 'Moscow_Mule',\n",
       " 'Predators',\n",
       " 'Private_Practice',\n",
       " 'Red_Eye',\n",
       " 'The_Glass_House',\n",
       " 'Apollo_13_(Film)',\n",
       " 'Bill_Ramsey',\n",
       " 'Hospital_Emergency_Codes',\n",
       " 'Safe_House',\n",
       " 'Raps',\n",
       " 'Catherine_O’Hara',\n",
       " 'Vincent_D’Onofrio',\n",
       " 'Haris_Seferović',\n",
       " 'AKB48',\n",
       " 'All_your_base_are_belong_to_us',\n",
       " 'Smells_Like_Teen_Spirit',\n",
       " 'All_your_base_are_belong_to_us',\n",
       " 'Goodbye_Mr._Black',\n",
       " 'Hotel_King',\n",
       " 'Monstar',\n",
       " 'Netflix',\n",
       " 'Oh_My_Venus',\n",
       " 'Undertale',\n",
       " 'Autobiographie',\n",
       " 'Deepwater_Horizon',\n",
       " 'Gedit',\n",
       " 'Jason_Bourne_(film)',\n",
       " '2016',\n",
       " 'Dragon_Ball_Super',\n",
       " 'VLC_media_player',\n",
       " 'Le_Bourgeois_gentilhomme',\n",
       " 'Space_Oddity',\n",
       " 'Airlift_(film)',\n",
       " 'Around_the_World_in_a_Day',\n",
       " 'Auld_Lang_Syne',\n",
       " 'Azhar_(film)',\n",
       " 'Baaghi_(2016_film)',\n",
       " 'Baar_Baar_Dekho',\n",
       " 'Bairavaa',\n",
       " 'Bound_2',\n",
       " 'Controversy_(Prince_album)',\n",
       " 'Dear_Zindagi',\n",
       " 'Descendants_of_the_Sun',\n",
       " 'Diamond_Dogs',\n",
       " 'Dilwale_(2015_film)',\n",
       " \"Don't_Stop_Me_Now\",\n",
       " 'English_alphabet',\n",
       " 'Fishsticks_(South_Park)',\n",
       " 'Fitoor',\n",
       " 'For_You_(Prince_album)',\n",
       " 'Freaky_Ali',\n",
       " 'Giorgio_by_Moroder',\n",
       " 'Greek_alphabet',\n",
       " 'Hurt_(Nine_Inch_Nails_song)',\n",
       " 'Hwarang:_The_Poet_Warrior_Youth',\n",
       " 'I_Am_the_Walrus',\n",
       " 'I_Can_Hear_Your_Voice',\n",
       " 'Jai_Gangaajal',\n",
       " 'Janatha_Garage',\n",
       " 'Jolly_LLB_2',\n",
       " 'Jonestown',\n",
       " 'Kaabil',\n",
       " 'Kahaani_2:_Durga_Rani_Singh',\n",
       " 'Ki_&_Ka',\n",
       " 'Kyaa_Kool_Hain_Hum_3',\n",
       " 'La_Cucaracha',\n",
       " 'Love_in_the_Moonlight',\n",
       " 'Mastizaade',\n",
       " 'Mohenjo_Daro_(film)',\n",
       " 'NATO_phonetic_alphabet',\n",
       " 'Olympic_flame',\n",
       " 'Parade_(Prince_album)',\n",
       " 'Perfect_Illusion',\n",
       " 'Peter_principle',\n",
       " 'Planet_Earth_II',\n",
       " 'Prince_(album)',\n",
       " 'Pulimurugan',\n",
       " 'Raaz:_Reboot',\n",
       " 'Rocky_Handsome',\n",
       " 'Salsa_Tequila',\n",
       " 'Sanam_Re',\n",
       " 'Spy_Hard_(song)',\n",
       " 'The_Day_the_Music_Died',\n",
       " 'The_K2',\n",
       " 'The_Legend_of_the_Blue_Sea',\n",
       " 'The_Real_Slim_Shady',\n",
       " 'This_Is_What_You_Came_For',\n",
       " 'Tunak_Tunak_Tun',\n",
       " 'Udta_Punjab',\n",
       " 'United_States_Secretary_of_State',\n",
       " 'United_States_presidential_inauguration',\n",
       " 'W_(TV_series)',\n",
       " 'We_Are_the_World',\n",
       " 'Weightlifting_Fairy_Kim_Bok-joo',\n",
       " 'Cigarettes_and_Valentines',\n",
       " 'Dhruva_(2016_film)',\n",
       " \"Five_Nights_at_Freddy's\",\n",
       " 'Happy_Bhag_Jayegi',\n",
       " 'Impeachment_of_Bill_Clinton',\n",
       " 'Iru_Mugan',\n",
       " 'Krampus',\n",
       " 'List_of_United_States_federal_executive_orders',\n",
       " 'Moon_Lovers:_Scarlet_Heart_Ryeo',\n",
       " 'Neerja',\n",
       " 'Never_Gonna_Give_You_Up',\n",
       " 'Ok_Jaanu',\n",
       " 'Si3_(film)',\n",
       " 'Smells_Like_Nirvana',\n",
       " 'Station_to_Station',\n",
       " 'Suicide_Is_Painless',\n",
       " 'Toys_in_the_Attic_(album)',\n",
       " 'Uncontrollably_Fond',\n",
       " 'Warzone_2100',\n",
       " 'Befikre',\n",
       " 'Closer_(The_Chainsmokers_song)',\n",
       " 'David_Bowie_(1967_album)',\n",
       " 'Devi_(2016_film)',\n",
       " 'Endless_(Frank_Ocean_album)',\n",
       " \"Godwin's_law\",\n",
       " 'Line_of_Duty',\n",
       " 'Lodger_(album)',\n",
       " 'Manchester_City_F.C.',\n",
       " 'Philip_Hammond',\n",
       " 'Pillowtalk_(song)',\n",
       " 'Rustom_(film)',\n",
       " 'Spaghetti-tree_hoax',\n",
       " 'Te3n',\n",
       " 'Xenu',\n",
       " '1944_(song)',\n",
       " 'Akira_(2016_Hindi_film)',\n",
       " 'American_Airlines_Flight_191',\n",
       " 'British_Empire',\n",
       " 'Class_(2016_TV_series)',\n",
       " 'Fentanyl',\n",
       " 'Guardian:_The_Lonely_and_Great_God',\n",
       " 'Housefull_3',\n",
       " 'Kapoor_&_Sons',\n",
       " 'Wazir_(film)',\n",
       " 'Bicycle_Race',\n",
       " 'Omayra_Sánchez',\n",
       " 'Sex',\n",
       " 'Grand_Theft_Auto_V',\n",
       " 'GNU_Free_Documentation_License',\n",
       " 'Mojito',\n",
       " 'Ne_me_quitte_pas',\n",
       " 'La_Brabançonne',\n",
       " '1920_London',\n",
       " 'Badrinath_Ki_Dulhania',\n",
       " 'Love_Games_(film)',\n",
       " 'Nothing_Compares_2_U',\n",
       " 'Prem_Ratan_Dhan_Payo',\n",
       " 'Raman_Raghav_2.0',\n",
       " 'Teraa_Surroor',\n",
       " 'XXX_(ZZ_Top_album)',\n",
       " 'Flash_(Barry_Allen)',\n",
       " 'Iraivi',\n",
       " 'Cicada',\n",
       " 'David_Tennant',\n",
       " 'Hindenburg_disaster',\n",
       " 'Smells_Like_Teen_Spirit',\n",
       " 'Do_Lafzon_Ki_Kahani_(film)',\n",
       " 'Force_2',\n",
       " 'Kaashmora',\n",
       " 'Bogan_(film)',\n",
       " 'Sarrainodu',\n",
       " 'Apollo_13',\n",
       " 'Bukkake',\n",
       " 'Burlesque',\n",
       " 'Durian',\n",
       " 'Joule',\n",
       " 'Kristin_Otto',\n",
       " 'O_Tannenbaum',\n",
       " 'Simmer_Down',\n",
       " 'Deutsche_Nationalhymne',\n",
       " 'Silver_Linings',\n",
       " 'The_Sound_of_Silence',\n",
       " 'Wacken_Open_Air',\n",
       " 'Fack_ju_Göhte',\n",
       " 'Schöne_Bescherung',\n",
       " 'Eisbär',\n",
       " 'H',\n",
       " 'Ich_bin_der_Doktor_Eisenbart',\n",
       " 'Süßer_die_Glocken_nie_klingen',\n",
       " 'Autoestima',\n",
       " 'Electricidad',\n",
       " 'Fuerza',\n",
       " 'Terremoto',\n",
       " 'Vitamina',\n",
       " 'Dream_High_2',\n",
       " 'Moondance',\n",
       " 'El_hotel_de_los_secretos',\n",
       " 'Himno_Nacional_Mexicano',\n",
       " 'Moneda',\n",
       " 'Teléfono',\n",
       " 'Sin_senos_sí_hay_paraíso',\n",
       " 'A_Aa',\n",
       " 'Bajrangi_Bhaijaan',\n",
       " 'Danny_Baker',\n",
       " 'Junooniyat',\n",
       " '69_(sex_position)',\n",
       " 'Gautamiputra_Satakarni_(film)',\n",
       " 'Hello_(Adele_song)',\n",
       " 'Jolly_LLB',\n",
       " 'Barbershop:_The_Next_Cut',\n",
       " 'Ishqbaaaz',\n",
       " 'List_of_common_misconceptions',\n",
       " 'A',\n",
       " 'Helicobacter_pylori',\n",
       " 'La_vecina',\n",
       " 'Testosterona',\n",
       " 'Golpe_de_Estado',\n",
       " 'My_Love_from_the_Star',\n",
       " '...The_Stories_We_Could_Tell',\n",
       " '50_Miniatures_for_Improvising_Quintet',\n",
       " 'A_Little_Bit_of_Jazz',\n",
       " 'Acid_rock',\n",
       " 'Back_on_the_Streets_(song)',\n",
       " \"Bambi's_Dilemma\",\n",
       " 'Better_Use_Your_Head',\n",
       " 'Big_Yellow_Taxi',\n",
       " 'Bill_Gates_Must_Die',\n",
       " 'Bizounce',\n",
       " 'Borbetomagus_(1982_album)',\n",
       " 'Buckshot_LeFonque_(album)',\n",
       " 'Burn_(Ellie_Goulding_song)',\n",
       " 'Call_Off_the_Search_(song)',\n",
       " 'Chirutha',\n",
       " 'Committed_to_the_Crime',\n",
       " 'Conglomerate_International',\n",
       " 'Construct-Destruct',\n",
       " 'Crac!',\n",
       " 'Crazy_Cukkad_Family',\n",
       " 'Crwn_Thy_Frnicatr',\n",
       " 'DJ_(Alphabeat_song)',\n",
       " 'Daughter_of_Eve',\n",
       " 'Denikaina_Ready',\n",
       " 'Desperate_Youth,_Blood_Thirsty_Babes',\n",
       " 'Drive:_Glay_Complete_Best',\n",
       " 'Drop_the_Pilot',\n",
       " 'Drunk_Enough_to_Dance',\n",
       " 'Electric_Psalmbook',\n",
       " 'Empty_&_Beautiful',\n",
       " 'Every_Picture_Tells_a_Story',\n",
       " 'False_Cathedrals',\n",
       " 'Feel_(George_Duke_album)',\n",
       " 'Galaxian_(album)',\n",
       " 'Getto_Jam',\n",
       " 'Greatest_&_Latest_(Warrant_album)',\n",
       " 'Greatest_Hits_(Chilliwack_album)',\n",
       " 'Grinderman_(album)',\n",
       " \"Gunnin'_for_Glory\",\n",
       " 'Hazy_Eyes',\n",
       " 'Hikari,_Hikaru',\n",
       " 'How_Can_I_Unlove_You',\n",
       " 'How_Great_Is_Our_God',\n",
       " 'I_Dig_Love',\n",
       " \"I_Don't_Play_That\",\n",
       " \"I_Won't_Let_You_Down_(Ph.D._song)\",\n",
       " 'Impressions_of_the_West_Lake',\n",
       " 'In_the_Air_(True_Tiger_song)',\n",
       " 'Invite_the_Light',\n",
       " 'Is_That_a_Tear',\n",
       " 'Jidaishin',\n",
       " 'Just_the_Way_You_Are_(Milky_song)',\n",
       " 'Kabhi_Kabhie_Mere_Dil_Mein',\n",
       " 'Keep_Smiling_(album)',\n",
       " 'Kucch_Luv_Jaisaa',\n",
       " 'Kuka_teki_huorin',\n",
       " 'Lana_Del_Ray_(album)',\n",
       " 'Lap_of_Honour',\n",
       " 'Lethal_Weapon_(soundtrack)',\n",
       " 'Live_at_the_Folklore_Center_1967',\n",
       " 'Live_in_Hollywood_(RBD_album)',\n",
       " 'Love_in_This_Club',\n",
       " 'Maggots:_The_Record',\n",
       " 'Make_Believe_(Pineforest_Crunch_album)',\n",
       " 'Megalomania_(Enslavement_of_Beauty_album)',\n",
       " 'Menteur',\n",
       " 'Miss_Sunshine',\n",
       " 'Missing_You_Now',\n",
       " 'Most_Precious_Love',\n",
       " 'Mumbai-Pune-Mumbai',\n",
       " 'New_and_Improved_(The_Spinners_album)',\n",
       " 'Night_Light_(TV_series)',\n",
       " 'Not_Counting_You',\n",
       " 'Nothing_Is_Easy:_Live_at_the_Isle_of_Wight_1970',\n",
       " 'Oborozukiyo:_Inori',\n",
       " 'Off_the_Record_(My_Morning_Jacket_song)',\n",
       " 'Oi_Oi_Oi_(Remixed)',\n",
       " 'Old_Low_Light',\n",
       " 'On_the_Ride',\n",
       " 'Our_Will_Be_Done',\n",
       " 'Out_of_the_Dark_(O.G._Funk_album)',\n",
       " 'Piriyadha_Varam_Vendum',\n",
       " 'Plastic_Dreams_(album)',\n",
       " 'Pluto_3D',\n",
       " 'Position_Correction',\n",
       " 'Power_of_the_Press',\n",
       " 'Pray_Along_with_Little_Richard_(Vol_2)',\n",
       " 'Pretty_Ugly',\n",
       " 'Puerto_Rican_amazon',\n",
       " 'Randy_Rogers_Band_(album)',\n",
       " 'Rhythm_Is_My_Business',\n",
       " \"Rockin'_the_Boat\",\n",
       " 'Sarah_Huckabee_Sanders',\n",
       " 'Shame_on_You_(The_Native_Years)',\n",
       " 'Shine_(Luther_Vandross_song)',\n",
       " 'Sick_of_Waiting_Tables',\n",
       " \"Sippin'_on_Fire\",\n",
       " \"Skinny_(They_Can't_Get_Enough)\",\n",
       " 'Sleep_in_Your_Grave',\n",
       " 'Someday_(Crystal_Gayle_album)',\n",
       " 'Songs_from_the_British_Isles',\n",
       " 'Speed_of_Sound_(song)',\n",
       " 'Spine_and_Sensory',\n",
       " 'Spiral_Live_at_Montreux_1978',\n",
       " 'Stahlmania',\n",
       " \"Steamin'_with_the_Miles_Davis_Quintet\",\n",
       " \"Steppin'_with_the_World_Saxophone_Quartet\",\n",
       " 'Street_Tough',\n",
       " 'Swing_Low,_Sweet_Cadillac',\n",
       " 'Ta_Oreotera_Tragoudia_Mou',\n",
       " 'Take_the_Box',\n",
       " 'The_Best_of_Steve_Harley_and_Cockney_Rebel',\n",
       " 'The_Best_of_Tracy_Lawrence',\n",
       " 'The_Bird_in_the_Bush_(Traditional_Erotic_Songs)',\n",
       " 'The_Cure:_Trilogy',\n",
       " 'The_First_Collage',\n",
       " 'The_Italian_(album)',\n",
       " 'The_Naked_Carmen',\n",
       " 'The_Night_the_Lights_Went_Out_in_Georgia',\n",
       " 'The_Performer_(Marty_Robbins_album)',\n",
       " 'The_Safety_Dance',\n",
       " 'The_Toughest',\n",
       " 'The_Venus_Trail',\n",
       " 'The_Wayward_Bus_(album)',\n",
       " \"There's_a_Place_for_Us_(song)\",\n",
       " 'Timber_Timbre_(album)',\n",
       " 'Time_Is_Time',\n",
       " 'Titan_(album)',\n",
       " 'Together_Again_(The_Dubliners_album)',\n",
       " 'Trees_and_Grass_and_Things',\n",
       " 'Tropical_Postcards',\n",
       " 'True_Magic',\n",
       " 'Unchain_My_Heart_(song)',\n",
       " 'Undress_to_the_Beat',\n",
       " 'Unleashed_Memories',\n",
       " 'Walk_(Grayson/Reed_EP)',\n",
       " 'When_You_Need_Me_(Aaron_Hall_song)',\n",
       " 'With_a_Little_Help_from_My_Fwends',\n",
       " 'Xmas_Time_of_the_Year',\n",
       " \"You're_the_World_to_Me\",\n",
       " 'You_Boyz_Make_Big_Noize',\n",
       " 'Pandorum',\n",
       " 'Anfield',\n",
       " 'Gezeiten',\n",
       " 'Völkerball',\n",
       " 'BLEACH',\n",
       " 'NBA',\n",
       " 'ONE_PIECE',\n",
       " 'American_Standard_Code_for_Information_Interchange',\n",
       " 'Bernard_Madoff',\n",
       " 'Homo_sapiens',\n",
       " 'Leptoglossus_occidentalis',\n",
       " 'Loxosceles_reclusa',\n",
       " 'Mohandas_Karamchand_Gandhi',\n",
       " 'Pupille_de_la_Nation',\n",
       " 'Rhinolophus_hipposideros',\n",
       " 'Shoah',\n",
       " 'Tardigrada',\n",
       " 'Universal_Serial_Bus',\n",
       " 'Air',\n",
       " 'Champignon',\n",
       " 'Concours_Eurovision_de_la_chanson',\n",
       " 'Everest',\n",
       " 'Internet_Movie_Database',\n",
       " 'JavaScript_Object_Notation',\n",
       " \"Jeanne_d'Arc\",\n",
       " 'Jenifer',\n",
       " 'Meghan_Markle',\n",
       " 'World_Wrestling_Entertainment',\n",
       " \"Côte_d'Ivoire\",\n",
       " 'Cobra_effect',\n",
       " 'FARC',\n",
       " 'Global_warming',\n",
       " 'List_of_Doctor_Who_serials',\n",
       " 'OS_X',\n",
       " 'United_States_presidential_elections_in_which_the_winner_lost_the_popular_vote',\n",
       " 'Zoe_Saldana',\n",
       " 'd-block',\n",
       " 'Charles,_Prince_of_Wales',\n",
       " 'Lewinsky_scandal',\n",
       " 'Neel_Sethi',\n",
       " 'The_Secret_Service_(comics)',\n",
       " 'United_States_Constitution',\n",
       " 'Killing_of_Harambe',\n",
       " 'International_Standard_Book_Number',\n",
       " 'Saúl_Álvarez',\n",
       " 'Seattle,_Washington',\n",
       " 'Turdus_migratorius',\n",
       " 'Erythrura_gouldiae',\n",
       " 'Wikia',\n",
       " 'I18n',\n",
       " 'Acari',\n",
       " 'Kill_Bill',\n",
       " 'Litchi',\n",
       " 'Yolanda_Foster',\n",
       " 'Bollywood',\n",
       " 'Adipositas',\n",
       " 'Aviator',\n",
       " 'Daimler_AG',\n",
       " 'Deutschland',\n",
       " 'Diabetes_mellitus',\n",
       " 'God_Save_the_Queen',\n",
       " 'Hillary_Rodham_Clinton',\n",
       " 'Josef_Stalin',\n",
       " 'Luxemburg',\n",
       " 'Michael_Balzary',\n",
       " 'Mohammed',\n",
       " 'The_Voice_Kids',\n",
       " 'Wien',\n",
       " 'Anis_Amri',\n",
       " 'Antonym',\n",
       " 'Glock_17',\n",
       " 'Hannover',\n",
       " 'Holocaust',\n",
       " 'Logarithmus',\n",
       " 'Putsch',\n",
       " 'Schweden',\n",
       " 'Generation_Y',\n",
       " 'Gravitation',\n",
       " 'Napoleon_Bonaparte',\n",
       " 'Osho',\n",
       " 'Bruce_Jenner',\n",
       " 'DEATH_NOTE',\n",
       " 'ISIL',\n",
       " 'PUFFY',\n",
       " 'LiSA',\n",
       " 'KARA',\n",
       " 'Big_Freeze',\n",
       " 'British_Airways_i360',\n",
       " 'Nationalsozialismus',\n",
       " 'Murgantia_histrionica',\n",
       " 'Animalia',\n",
       " 'Aves',\n",
       " 'Canis_lupus_familiaris',\n",
       " 'Danaus_plexippus',\n",
       " ...]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_to_articles = defaultdict(list)\n",
    "for category, topic in category_to_topic.items():\n",
    "    topic_to_articles[topic].extend(cat_to_articles[category])\n",
    "\n",
    "topic_to_articles[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f8dcca2-f53c-4bec-bcc8-27cd8746993f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Biographical Information and Entertainment',\n",
       " 1: 'Articles Needing References and Identifiers',\n",
       " 2: 'Television Series and Science Fiction',\n",
       " 3: 'Films and Literature',\n",
       " 4: 'African American and European Establishments',\n",
       " 5: 'Geographical Locations and Establishments',\n",
       " 6: 'Deaths, Actors, and Sports',\n",
       " 7: 'External Links and History',\n",
       " 8: 'Geography and Sports Stubs',\n",
       " 9: 'Writers and Politicians',\n",
       " 10: 'Articles Containing Text and Statements',\n",
       " 11: 'Albums and University Establishments',\n",
       " 12: 'Songs and Sports Events',\n",
       " 13: 'Fictional Characters and Competitions',\n",
       " 14: 'Players and Structures',\n",
       " 15: 'People and Football',\n",
       " 16: 'Software and Political Information',\n",
       " 17: 'Redirects and Films',\n",
       " 18: 'People of Various Descent',\n",
       " 19: 'United States and Businesspeople'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b122986-8921-4722-9e47-037cfdbbe702",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
