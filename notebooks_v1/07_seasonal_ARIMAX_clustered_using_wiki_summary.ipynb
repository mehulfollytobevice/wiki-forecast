{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43494b61-ad96-4866-8a3b-4948ea43cc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7a05a63-4b47-4b95-8aa3-fde21f16c6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Non-stationary starting autoregressive parameters found\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Non-invertible starting MA parameters found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30b207a0-ab44-4455-9352-29038f1f3ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import ARTICLE_SUMMARIES_FILE, SARIMAX_SUMMARY_CLUSTER_DIR\n",
    "from meta_data_utils import sanitize_filename, get_meta_data, get_cat_to_articles\n",
    "from data_utils import load_data, get_date_columns, check_stationarity, get_page_to_article_domain_mapping, filter_articles, get_page_info_df\n",
    "from plot_utils import plot_stacked_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e7d6d1c-3053-474e-a37a-679cc6aa77e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = load_data(transpose=False, remove_inactive_articles=True)\n",
    "date_columns = get_date_columns(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e4b71e7-c2b0-4e41-af39-7910c170c87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_info_df = get_page_info_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d62b3121-cf0e-4624-bda1-d7151e95363c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>page</th>\n",
       "      <th>2NE1_zh.wikipedia.org_all-access_spider</th>\n",
       "      <th>2PM_zh.wikipedia.org_all-access_spider</th>\n",
       "      <th>3C_zh.wikipedia.org_all-access_spider</th>\n",
       "      <th>4minute_zh.wikipedia.org_all-access_spider</th>\n",
       "      <th>5566_zh.wikipedia.org_all-access_spider</th>\n",
       "      <th>A'N'D_zh.wikipedia.org_all-access_spider</th>\n",
       "      <th>AKB48_zh.wikipedia.org_all-access_spider</th>\n",
       "      <th>ASCII_zh.wikipedia.org_all-access_spider</th>\n",
       "      <th>Ahq_e-Sports_Club_zh.wikipedia.org_all-access_spider</th>\n",
       "      <th>All_your_base_are_belong_to_us_zh.wikipedia.org_all-access_spider</th>\n",
       "      <th>...</th>\n",
       "      <th>Transgénero_es.wikipedia.org_all-access_spider</th>\n",
       "      <th>Edad_Contemporánea_es.wikipedia.org_all-access_spider</th>\n",
       "      <th>Salvador_Dalí_es.wikipedia.org_all-access_spider</th>\n",
       "      <th>Soraya_Jiménez_es.wikipedia.org_all-access_spider</th>\n",
       "      <th>Día_Internacional_del_Beso_es.wikipedia.org_all-access_spider</th>\n",
       "      <th>Chichén_Itzá_es.wikipedia.org_all-access_spider</th>\n",
       "      <th>Fecundación_es.wikipedia.org_all-access_spider</th>\n",
       "      <th>Gran_Hermano_VIP_(España)_es.wikipedia.org_all-access_spider</th>\n",
       "      <th>Modelo_atómico_de_Thomson_es.wikipedia.org_all-access_spider</th>\n",
       "      <th>Copa_América_2019_es.wikipedia.org_all-access_spider</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-07-01</th>\n",
       "      <td>18.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-02</th>\n",
       "      <td>11.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-03</th>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-04</th>\n",
       "      <td>13.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-05</th>\n",
       "      <td>14.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 115084 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "page        2NE1_zh.wikipedia.org_all-access_spider  \\\n",
       "2015-07-01                                     18.0   \n",
       "2015-07-02                                     11.0   \n",
       "2015-07-03                                      5.0   \n",
       "2015-07-04                                     13.0   \n",
       "2015-07-05                                     14.0   \n",
       "\n",
       "page        2PM_zh.wikipedia.org_all-access_spider  \\\n",
       "2015-07-01                                    11.0   \n",
       "2015-07-02                                    14.0   \n",
       "2015-07-03                                    15.0   \n",
       "2015-07-04                                    18.0   \n",
       "2015-07-05                                    11.0   \n",
       "\n",
       "page        3C_zh.wikipedia.org_all-access_spider  \\\n",
       "2015-07-01                                    1.0   \n",
       "2015-07-02                                    0.0   \n",
       "2015-07-03                                    1.0   \n",
       "2015-07-04                                    1.0   \n",
       "2015-07-05                                    0.0   \n",
       "\n",
       "page        4minute_zh.wikipedia.org_all-access_spider  \\\n",
       "2015-07-01                                        35.0   \n",
       "2015-07-02                                        13.0   \n",
       "2015-07-03                                        10.0   \n",
       "2015-07-04                                        94.0   \n",
       "2015-07-05                                         4.0   \n",
       "\n",
       "page        5566_zh.wikipedia.org_all-access_spider  \\\n",
       "2015-07-01                                     12.0   \n",
       "2015-07-02                                      7.0   \n",
       "2015-07-03                                      4.0   \n",
       "2015-07-04                                      5.0   \n",
       "2015-07-05                                     20.0   \n",
       "\n",
       "page        A'N'D_zh.wikipedia.org_all-access_spider  \\\n",
       "2015-07-01                                     118.0   \n",
       "2015-07-02                                      26.0   \n",
       "2015-07-03                                      30.0   \n",
       "2015-07-04                                      24.0   \n",
       "2015-07-05                                      29.0   \n",
       "\n",
       "page        AKB48_zh.wikipedia.org_all-access_spider  \\\n",
       "2015-07-01                                       5.0   \n",
       "2015-07-02                                      23.0   \n",
       "2015-07-03                                      14.0   \n",
       "2015-07-04                                      12.0   \n",
       "2015-07-05                                       9.0   \n",
       "\n",
       "page        ASCII_zh.wikipedia.org_all-access_spider  \\\n",
       "2015-07-01                                       6.0   \n",
       "2015-07-02                                       3.0   \n",
       "2015-07-03                                       5.0   \n",
       "2015-07-04                                      12.0   \n",
       "2015-07-05                                       6.0   \n",
       "\n",
       "page        Ahq_e-Sports_Club_zh.wikipedia.org_all-access_spider  \\\n",
       "2015-07-01                                                2.0      \n",
       "2015-07-02                                                1.0      \n",
       "2015-07-03                                                4.0      \n",
       "2015-07-04                                                4.0      \n",
       "2015-07-05                                                2.0      \n",
       "\n",
       "page        All_your_base_are_belong_to_us_zh.wikipedia.org_all-access_spider  \\\n",
       "2015-07-01                                                2.0                   \n",
       "2015-07-02                                                5.0                   \n",
       "2015-07-03                                                5.0                   \n",
       "2015-07-04                                                1.0                   \n",
       "2015-07-05                                                3.0                   \n",
       "\n",
       "page        ...  Transgénero_es.wikipedia.org_all-access_spider  \\\n",
       "2015-07-01  ...                                             4.0   \n",
       "2015-07-02  ...                                             3.0   \n",
       "2015-07-03  ...                                             4.0   \n",
       "2015-07-04  ...                                            17.0   \n",
       "2015-07-05  ...                                            28.0   \n",
       "\n",
       "page        Edad_Contemporánea_es.wikipedia.org_all-access_spider  \\\n",
       "2015-07-01                                               21.0       \n",
       "2015-07-02                                               32.0       \n",
       "2015-07-03                                               38.0       \n",
       "2015-07-04                                               21.0       \n",
       "2015-07-05                                               28.0       \n",
       "\n",
       "page        Salvador_Dalí_es.wikipedia.org_all-access_spider  \\\n",
       "2015-07-01                                              23.0   \n",
       "2015-07-02                                              40.0   \n",
       "2015-07-03                                              55.0   \n",
       "2015-07-04                                              32.0   \n",
       "2015-07-05                                              42.0   \n",
       "\n",
       "page        Soraya_Jiménez_es.wikipedia.org_all-access_spider  \\\n",
       "2015-07-01                                                3.0   \n",
       "2015-07-02                                                7.0   \n",
       "2015-07-03                                                6.0   \n",
       "2015-07-04                                                3.0   \n",
       "2015-07-05                                                3.0   \n",
       "\n",
       "page        Día_Internacional_del_Beso_es.wikipedia.org_all-access_spider  \\\n",
       "2015-07-01                                                1.0               \n",
       "2015-07-02                                                3.0               \n",
       "2015-07-03                                                8.0               \n",
       "2015-07-04                                                3.0               \n",
       "2015-07-05                                                6.0               \n",
       "\n",
       "page        Chichén_Itzá_es.wikipedia.org_all-access_spider  \\\n",
       "2015-07-01                                              8.0   \n",
       "2015-07-02                                             13.0   \n",
       "2015-07-03                                             19.0   \n",
       "2015-07-04                                             14.0   \n",
       "2015-07-05                                              6.0   \n",
       "\n",
       "page        Fecundación_es.wikipedia.org_all-access_spider  \\\n",
       "2015-07-01                                            29.0   \n",
       "2015-07-02                                            16.0   \n",
       "2015-07-03                                             6.0   \n",
       "2015-07-04                                            11.0   \n",
       "2015-07-05                                            33.0   \n",
       "\n",
       "page        Gran_Hermano_VIP_(España)_es.wikipedia.org_all-access_spider  \\\n",
       "2015-07-01                                                4.0              \n",
       "2015-07-02                                               25.0              \n",
       "2015-07-03                                                7.0              \n",
       "2015-07-04                                               11.0              \n",
       "2015-07-05                                                6.0              \n",
       "\n",
       "page        Modelo_atómico_de_Thomson_es.wikipedia.org_all-access_spider  \\\n",
       "2015-07-01                                                0.0              \n",
       "2015-07-02                                                2.0              \n",
       "2015-07-03                                                6.0              \n",
       "2015-07-04                                                6.0              \n",
       "2015-07-05                                                7.0              \n",
       "\n",
       "page        Copa_América_2019_es.wikipedia.org_all-access_spider  \n",
       "2015-07-01                                                3.0     \n",
       "2015-07-02                                               10.0     \n",
       "2015-07-03                                               41.0     \n",
       "2015-07-04                                               17.0     \n",
       "2015-07-05                                               16.0     \n",
       "\n",
       "[5 rows x 115084 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merged_df = pd.merge(page_info_df[['page', 'article']], raw_data, on='page', how='inner')\n",
    "# merged_df.drop('', axis=1, inplace=True)\n",
    "df = raw_data.copy()\n",
    "df.set_index('page', inplace=True)\n",
    "df = df.T\n",
    "df.index = pd.to_datetime(df.index)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb59963c-4719-4957-957f-d0b993806580",
   "metadata": {},
   "source": [
    "### Get Wikipedia Article Summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1831b0e8-ba2d-4725-a92d-317d0ff0dea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_df = pd.read_csv(ARTICLE_SUMMARIES_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4249fa5a-5a9d-4e83-b228-73e91e396e3d",
   "metadata": {},
   "source": [
    "### Topic Modeling and Group Similar Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "722ab622-ad7c-4fe1-85dc-1c0d59f62228",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ajaykarthicksenthilkumar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: nan, refer, python, アメリカ合衆国の俳優, みゆき, 美智子, strikeforce, xxxx, клуб, 健太郎\n",
      "\n",
      "\n",
      "Topic 1: nba, golf, élection, ali, reagan, hudson, lakers, italian, yves, tor\n",
      "\n",
      "\n",
      "Topic 2: series, american, film, television, created, known, written, published, based, directed\n",
      "\n",
      "\n",
      "Topic 3: potter, harry, икс, google, rowling, technology, князь, computing, company, 美奈子\n",
      "\n",
      "\n",
      "Topic 4: footballer, professional, plays, midfielder, club, played, attacking, актриса, striker, team\n",
      "\n",
      "\n",
      "Topic 5: фильм, американский, режиссёра, роли, фильма, фильме, телесериал, года, роль, также\n",
      "\n",
      "\n",
      "Topic 6: president, party, politician, served, united, states, minister, member, democratic, political\n",
      "\n",
      "\n",
      "Topic 7: der, tag, kalenders, gregorianischen, jahresende, tage, somit, bleiben, zum, bis\n",
      "\n",
      "\n",
      "Topic 8: que, los, del, una, por, las, como, para, fue, más\n",
      "\n",
      "\n",
      "Topic 9: juegos, 日本の俳優, 日本の女優, olímpicos, refer, mediawiki, tipo, trump, movimiento, wikimedia\n",
      "\n",
      "\n",
      "Topic 10: manga, shōnen, tankōbon, volumes, serialized, 日本の俳優タレント, chapters, collected, чемпионка, illustrated\n",
      "\n",
      "\n",
      "Topic 11: российский, кино, актёр, театра, refer, советский, российской, федерации, актриса, премии\n",
      "\n",
      "\n",
      "Topic 12: tennis, singles, football, rio, équipe, janeiro, open, doubles, coupe, swimmer\n",
      "\n",
      "\n",
      "Topic 13: football, professional, team, league, world, player, national, played, club, olympic\n",
      "\n",
      "\n",
      "Topic 14: года, или, также, как, россии, году, является, мира, для, его\n",
      "\n",
      "\n",
      "Topic 15: award, awards, actor, actress, film, academy, golden, american, films, known\n",
      "\n",
      "\n",
      "Topic 16: singer, actress, known, film, actor, band, japanese, american, music, series\n",
      "\n",
      "\n",
      "Topic 17: jeux, olympiques, комиксов, comics, refer, издательства, marvel, akb48, used, xxx\n",
      "\n",
      "\n",
      "Topic 18: der, die, und, ist, von, ein, eine, den, dem, das\n",
      "\n",
      "\n",
      "Topic 19: est, une, city, par, les, des, country, population, dans, largest\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>domain</th>\n",
       "      <th>topic_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2NE1</td>\n",
       "      <td>zh.wikipedia.org</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2PM</td>\n",
       "      <td>zh.wikipedia.org</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3C</td>\n",
       "      <td>zh.wikipedia.org</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4minute</td>\n",
       "      <td>zh.wikipedia.org</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5566</td>\n",
       "      <td>zh.wikipedia.org</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article            domain  topic_id\n",
       "0     2NE1  zh.wikipedia.org        16\n",
       "1      2PM  zh.wikipedia.org        16\n",
       "2       3C  zh.wikipedia.org        11\n",
       "3  4minute  zh.wikipedia.org        16\n",
       "4     5566  zh.wikipedia.org        16"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Preprocessing \n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\b\\w{1,2}\\b', '', text)  # Remove short words\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "# Preprocess summaries\n",
    "summaries_df['processed_summary'] = summaries_df['summary'].apply(preprocess_text)\n",
    "\n",
    "# Vectorize the preprocessed summaries using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(summaries_df['processed_summary'])\n",
    "\n",
    "# Apply LDA to identify topics\n",
    "num_topics = 20  \n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(tfidf_matrix)\n",
    "\n",
    "# Get the topic distribution for each summary\n",
    "topic_distribution = lda.transform(tfidf_matrix)\n",
    "\n",
    "# Assign each summary to the most probable topic\n",
    "summaries_df['topic_id'] = np.argmax(topic_distribution, axis=1)\n",
    "\n",
    "# Group summaries by their assigned topic\n",
    "topic_to_articles = defaultdict(list)\n",
    "for idx, row in summaries_df.iterrows():\n",
    "    topic_to_articles[row['topic_id']].append((row['article'], row['domain']))\n",
    "\n",
    "# Function to get the top words for each topic\n",
    "def get_top_words(model, feature_names, n_top_words):\n",
    "    top_words = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words[topic_idx] = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "    return top_words\n",
    "\n",
    "# Get the feature names (words) from the TF-IDF vectorizer\n",
    "tf_feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get the top words for each topic\n",
    "top_words = get_top_words(lda, tf_feature_names, 10)\n",
    "\n",
    "# Print the top words for each topic\n",
    "for topic, words in top_words.items():\n",
    "    print(f\"Topic {topic}: {', '.join(words)}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "summaries_df[['article', 'domain', 'topic_id']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e04bb00d-e12d-43cc-a982-48748e8fda48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_page_topic_mappings(summary_df, page_info_df):\n",
    "    page_to_topic = {}\n",
    "    topic_to_page = defaultdict(list)\n",
    "\n",
    "    # Create a mapping from (article, domain) to topic_id\n",
    "    article_domain_to_topic = summary_df.set_index(['article', 'domain'])['topic_id'].to_dict()\n",
    "\n",
    "    # Create page_to_topic and topic_to_page mappings\n",
    "    for index, row in page_info_df.iterrows():\n",
    "        article = row['article']\n",
    "        domain = row['domain']\n",
    "        page = row['page']\n",
    "        \n",
    "        if (article, domain) in article_domain_to_topic:\n",
    "            topic_id = article_domain_to_topic[(article, domain)]\n",
    "            page_to_topic[page] = topic_id\n",
    "            topic_to_page[topic_id].append(page)\n",
    "\n",
    "    return page_to_topic, topic_to_page\n",
    "\n",
    "page_to_topic, topic_to_page = create_page_topic_mappings(summaries_df, page_info_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "334e2bf2-cda1-484d-9ead-41fbc594fd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [10:53<00:00, 32.66s/it]\n"
     ]
    }
   ],
   "source": [
    "def compute_within_cluster_variance(data_df, cluster_to_pages):\n",
    "    scores = defaultdict(dict)\n",
    "    scaler = StandardScaler()\n",
    "    # Convert data_df columns to a set for efficient look-up\n",
    "    valid_columns_set = set(data_df.columns)\n",
    "    \n",
    "    # Compute within-cluster variance for each cluster\n",
    "    for cluster_id, pages in tqdm(cluster_to_pages.items(), total=len(cluster_to_pages.items())):\n",
    "        # Filter the pages that are present in the DataFrame using set intersection\n",
    "        valid_pages = list(set(pages) & valid_columns_set)\n",
    "        \n",
    "        if len(valid_pages) < 10:\n",
    "            continue    # Need at least 10 pages to compute any meaningful measure\n",
    "\n",
    "        # Extract and standardize data\n",
    "        category_data = scaler.fit_transform(data_df[valid_pages].fillna(0).T)\n",
    "\n",
    "        # Use PCA to reduce dimensionality for large data sets\n",
    "        pca = PCA(n_components=min(category_data.shape) - 1 if category_data.shape[0] > 2 else 2)\n",
    "        reduced_data = pca.fit_transform(category_data)\n",
    "\n",
    "        # Calculate variance within the cluster\n",
    "        variance = np.var(reduced_data, axis=0).mean()  # Average variance across all components\n",
    "        scores[cluster_id] = [variance, len(valid_pages)]\n",
    "\n",
    "    return pd.DataFrame(scores, index=['Variance', 'Num_Pages']).T\n",
    "\n",
    "\n",
    "within_cluster_variance = compute_within_cluster_variance(df, topic_to_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e0d9950-d3a1-4b91-89c4-3f2b3716ce2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variance</th>\n",
       "      <th>Num_Pages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.001245</td>\n",
       "      <td>11335.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.001246</td>\n",
       "      <td>4279.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.001247</td>\n",
       "      <td>30007.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.001246</td>\n",
       "      <td>13409.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.001246</td>\n",
       "      <td>6898.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.001246</td>\n",
       "      <td>7050.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.001247</td>\n",
       "      <td>3551.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.001247</td>\n",
       "      <td>1310.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.001247</td>\n",
       "      <td>1048.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.001247</td>\n",
       "      <td>8865.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.001247</td>\n",
       "      <td>1299.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.001247</td>\n",
       "      <td>813.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.001247</td>\n",
       "      <td>1250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.001247</td>\n",
       "      <td>1748.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.001247</td>\n",
       "      <td>4413.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.001247</td>\n",
       "      <td>1537.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.001247</td>\n",
       "      <td>958.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.001246</td>\n",
       "      <td>5911.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.001246</td>\n",
       "      <td>5745.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.001247</td>\n",
       "      <td>928.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Variance  Num_Pages\n",
       "16  1.001245    11335.0\n",
       "11  1.001246     4279.0\n",
       "0   1.001247    30007.0\n",
       "2   1.001246    13409.0\n",
       "13  1.001246     6898.0\n",
       "15  1.001246     7050.0\n",
       "6   1.001247     3551.0\n",
       "12  1.001247     1310.0\n",
       "9   1.001247     1048.0\n",
       "19  1.001247     8865.0\n",
       "7   1.001247     1299.0\n",
       "3   1.001247      813.0\n",
       "17  1.001247     1250.0\n",
       "4   1.001247     1748.0\n",
       "14  1.001247     4413.0\n",
       "5   1.001247     1537.0\n",
       "10  1.001247      958.0\n",
       "18  1.001246     5911.0\n",
       "8   1.001246     5745.0\n",
       "1   1.001247      928.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "within_cluster_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88840884-8806-45a7-adc2-9fbcb570e7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:07<00:00,  2.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>16</th>\n",
       "      <th>11</th>\n",
       "      <th>0</th>\n",
       "      <th>2</th>\n",
       "      <th>13</th>\n",
       "      <th>15</th>\n",
       "      <th>6</th>\n",
       "      <th>12</th>\n",
       "      <th>9</th>\n",
       "      <th>19</th>\n",
       "      <th>7</th>\n",
       "      <th>3</th>\n",
       "      <th>17</th>\n",
       "      <th>4</th>\n",
       "      <th>14</th>\n",
       "      <th>5</th>\n",
       "      <th>10</th>\n",
       "      <th>18</th>\n",
       "      <th>8</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-07-01</th>\n",
       "      <td>10505023.0</td>\n",
       "      <td>1803024.0</td>\n",
       "      <td>67520051.0</td>\n",
       "      <td>12837786.0</td>\n",
       "      <td>6472189.0</td>\n",
       "      <td>7031516.0</td>\n",
       "      <td>2611946.0</td>\n",
       "      <td>937690.0</td>\n",
       "      <td>779223.0</td>\n",
       "      <td>6289589.0</td>\n",
       "      <td>813852.0</td>\n",
       "      <td>525632.0</td>\n",
       "      <td>709019.0</td>\n",
       "      <td>885449.0</td>\n",
       "      <td>1994919.0</td>\n",
       "      <td>717308.0</td>\n",
       "      <td>554581.0</td>\n",
       "      <td>2228062.0</td>\n",
       "      <td>4782959.0</td>\n",
       "      <td>477094.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-02</th>\n",
       "      <td>10625718.0</td>\n",
       "      <td>1757603.0</td>\n",
       "      <td>70127936.0</td>\n",
       "      <td>12599544.0</td>\n",
       "      <td>5699228.0</td>\n",
       "      <td>7106207.0</td>\n",
       "      <td>2479868.0</td>\n",
       "      <td>790215.0</td>\n",
       "      <td>670003.0</td>\n",
       "      <td>6022331.0</td>\n",
       "      <td>827066.0</td>\n",
       "      <td>573199.0</td>\n",
       "      <td>718597.0</td>\n",
       "      <td>1074759.0</td>\n",
       "      <td>1964479.0</td>\n",
       "      <td>744175.0</td>\n",
       "      <td>613675.0</td>\n",
       "      <td>2180176.0</td>\n",
       "      <td>4507640.0</td>\n",
       "      <td>480480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-03</th>\n",
       "      <td>10601401.0</td>\n",
       "      <td>1756415.0</td>\n",
       "      <td>64999228.0</td>\n",
       "      <td>12127624.0</td>\n",
       "      <td>5776448.0</td>\n",
       "      <td>7104477.0</td>\n",
       "      <td>2359061.0</td>\n",
       "      <td>754946.0</td>\n",
       "      <td>590168.0</td>\n",
       "      <td>5602059.0</td>\n",
       "      <td>696632.0</td>\n",
       "      <td>522498.0</td>\n",
       "      <td>695350.0</td>\n",
       "      <td>879761.0</td>\n",
       "      <td>1968846.0</td>\n",
       "      <td>697943.0</td>\n",
       "      <td>559213.0</td>\n",
       "      <td>2016670.0</td>\n",
       "      <td>3815199.0</td>\n",
       "      <td>464590.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-04</th>\n",
       "      <td>11613286.0</td>\n",
       "      <td>1880581.0</td>\n",
       "      <td>67395433.0</td>\n",
       "      <td>12707056.0</td>\n",
       "      <td>7069002.0</td>\n",
       "      <td>7593198.0</td>\n",
       "      <td>2420207.0</td>\n",
       "      <td>869897.0</td>\n",
       "      <td>599450.0</td>\n",
       "      <td>5841851.0</td>\n",
       "      <td>854808.0</td>\n",
       "      <td>536687.0</td>\n",
       "      <td>637668.0</td>\n",
       "      <td>960603.0</td>\n",
       "      <td>2047591.0</td>\n",
       "      <td>783103.0</td>\n",
       "      <td>620715.0</td>\n",
       "      <td>1982429.0</td>\n",
       "      <td>3189873.0</td>\n",
       "      <td>464309.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-05</th>\n",
       "      <td>11691856.0</td>\n",
       "      <td>1977211.0</td>\n",
       "      <td>68364039.0</td>\n",
       "      <td>13833106.0</td>\n",
       "      <td>7074307.0</td>\n",
       "      <td>8821296.0</td>\n",
       "      <td>2543752.0</td>\n",
       "      <td>714606.0</td>\n",
       "      <td>651333.0</td>\n",
       "      <td>6173256.0</td>\n",
       "      <td>735592.0</td>\n",
       "      <td>555786.0</td>\n",
       "      <td>664333.0</td>\n",
       "      <td>946967.0</td>\n",
       "      <td>2143456.0</td>\n",
       "      <td>844864.0</td>\n",
       "      <td>673249.0</td>\n",
       "      <td>2305704.0</td>\n",
       "      <td>3732327.0</td>\n",
       "      <td>518825.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    16         11          0           2          13  \\\n",
       "2015-07-01  10505023.0  1803024.0  67520051.0  12837786.0  6472189.0   \n",
       "2015-07-02  10625718.0  1757603.0  70127936.0  12599544.0  5699228.0   \n",
       "2015-07-03  10601401.0  1756415.0  64999228.0  12127624.0  5776448.0   \n",
       "2015-07-04  11613286.0  1880581.0  67395433.0  12707056.0  7069002.0   \n",
       "2015-07-05  11691856.0  1977211.0  68364039.0  13833106.0  7074307.0   \n",
       "\n",
       "                   15         6         12        9          19        7   \\\n",
       "2015-07-01  7031516.0  2611946.0  937690.0  779223.0  6289589.0  813852.0   \n",
       "2015-07-02  7106207.0  2479868.0  790215.0  670003.0  6022331.0  827066.0   \n",
       "2015-07-03  7104477.0  2359061.0  754946.0  590168.0  5602059.0  696632.0   \n",
       "2015-07-04  7593198.0  2420207.0  869897.0  599450.0  5841851.0  854808.0   \n",
       "2015-07-05  8821296.0  2543752.0  714606.0  651333.0  6173256.0  735592.0   \n",
       "\n",
       "                  3         17         4          14        5         10  \\\n",
       "2015-07-01  525632.0  709019.0   885449.0  1994919.0  717308.0  554581.0   \n",
       "2015-07-02  573199.0  718597.0  1074759.0  1964479.0  744175.0  613675.0   \n",
       "2015-07-03  522498.0  695350.0   879761.0  1968846.0  697943.0  559213.0   \n",
       "2015-07-04  536687.0  637668.0   960603.0  2047591.0  783103.0  620715.0   \n",
       "2015-07-05  555786.0  664333.0   946967.0  2143456.0  844864.0  673249.0   \n",
       "\n",
       "                   18         8         1   \n",
       "2015-07-01  2228062.0  4782959.0  477094.0  \n",
       "2015-07-02  2180176.0  4507640.0  480480.0  \n",
       "2015-07-03  2016670.0  3815199.0  464590.0  \n",
       "2015-07-04  1982429.0  3189873.0  464309.0  \n",
       "2015-07-05  2305704.0  3732327.0  518825.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_aggregated_df_with_proportions(data_df, topic_to_pages):\n",
    "    # Initialize an empty DataFrame with the same index as data_df\n",
    "    aggregated_df = pd.DataFrame(index=data_df.index)\n",
    "    \n",
    "    # Dictionary to store the proportion of each page in its topic aggregation\n",
    "    topic_proportions = {}\n",
    "    \n",
    "    # Convert data_df columns to a set for efficient look-up\n",
    "    valid_columns_set = set(data_df.columns)\n",
    "    \n",
    "    # Iterate over each topic and its pages\n",
    "    for topic_id, pages in tqdm(topic_to_pages.items(), total=len(topic_to_pages.items())):\n",
    "        # Filter the pages that are present in the DataFrame using set intersection\n",
    "        valid_pages = list(set(pages) & valid_columns_set)\n",
    "        \n",
    "        if not valid_pages:\n",
    "            continue\n",
    "        \n",
    "        # Sum the view counts of the valid pages for each day\n",
    "        aggregated_df[topic_id] = data_df[valid_pages].sum(axis=1)\n",
    "        \n",
    "        # Calculate the proportion of each page in the aggregation\n",
    "        total_views = data_df[valid_pages].sum().sum()\n",
    "        topic_proportions[topic_id] = {page: data_df[page].sum() / total_views for page in valid_pages}\n",
    "    \n",
    "    return aggregated_df, topic_proportions\n",
    "\n",
    "aggregated_df, topic_proportions = create_aggregated_df_with_proportions(df, topic_to_page)\n",
    "\n",
    "aggregated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14744e69-037b-411e-b72d-2e82dfe47edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_meta_data_for_topic(topic_to_pages):\n",
    "    topic_exog_features = {}\n",
    "\n",
    "    for topic, pages in tqdm(topic_to_pages.items(), total=len(topic_to_pages.items())):\n",
    "        meta_dfs = []\n",
    "\n",
    "        for page_name in pages:\n",
    "            meta_df = get_meta_data(page_name)\n",
    "            if meta_df is None:\n",
    "                continue\n",
    "            meta_dfs.append(meta_df)\n",
    "\n",
    "        # Aggregate meta data for all pages within a topic\n",
    "        if meta_dfs:\n",
    "            combined_meta_df = pd.concat(meta_dfs)\n",
    "            aggregated_meta_df = combined_meta_df.groupby('date').sum()\n",
    "            topic_exog_features[topic] = aggregated_meta_df\n",
    "\n",
    "    return topic_exog_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a00be9c0-0dc4-49b4-8d2c-6d1e992da10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|██████████████████▌                                                                                                                                                                       | 2/20 [01:03<08:50, 29.50s/it]/Users/ajaykarthicksenthilkumar/dev/personal/wiki-forcast/ajay/meta_data_utils.py:54: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df['end_of_day_size'] = meta_df['end_of_day_size'].replace(0, pd.NA).ffill()\n",
      "/Users/ajaykarthicksenthilkumar/dev/personal/wiki-forcast/ajay/meta_data_utils.py:54: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df['end_of_day_size'] = meta_df['end_of_day_size'].replace(0, pd.NA).ffill()\n",
      "/Users/ajaykarthicksenthilkumar/dev/personal/wiki-forcast/ajay/meta_data_utils.py:54: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df['end_of_day_size'] = meta_df['end_of_day_size'].replace(0, pd.NA).ffill()\n",
      "/Users/ajaykarthicksenthilkumar/dev/personal/wiki-forcast/ajay/meta_data_utils.py:54: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df['end_of_day_size'] = meta_df['end_of_day_size'].replace(0, pd.NA).ffill()\n",
      "/Users/ajaykarthicksenthilkumar/dev/personal/wiki-forcast/ajay/meta_data_utils.py:54: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df['end_of_day_size'] = meta_df['end_of_day_size'].replace(0, pd.NA).ffill()\n",
      "/Users/ajaykarthicksenthilkumar/dev/personal/wiki-forcast/ajay/meta_data_utils.py:54: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df['end_of_day_size'] = meta_df['end_of_day_size'].replace(0, pd.NA).ffill()\n",
      "/Users/ajaykarthicksenthilkumar/dev/personal/wiki-forcast/ajay/meta_data_utils.py:54: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df['end_of_day_size'] = meta_df['end_of_day_size'].replace(0, pd.NA).ffill()\n",
      "/Users/ajaykarthicksenthilkumar/dev/personal/wiki-forcast/ajay/meta_data_utils.py:54: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df['end_of_day_size'] = meta_df['end_of_day_size'].replace(0, pd.NA).ffill()\n",
      "/Users/ajaykarthicksenthilkumar/dev/personal/wiki-forcast/ajay/meta_data_utils.py:54: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df['end_of_day_size'] = meta_df['end_of_day_size'].replace(0, pd.NA).ffill()\n",
      " 25%|██████████████████████████████████████████████▌                                                                                                                                           | 5/20 [04:27<13:13, 52.87s/it]/Users/ajaykarthicksenthilkumar/dev/personal/wiki-forcast/ajay/meta_data_utils.py:54: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df['end_of_day_size'] = meta_df['end_of_day_size'].replace(0, pd.NA).ffill()\n",
      "/Users/ajaykarthicksenthilkumar/dev/personal/wiki-forcast/ajay/meta_data_utils.py:54: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df['end_of_day_size'] = meta_df['end_of_day_size'].replace(0, pd.NA).ffill()\n",
      "/Users/ajaykarthicksenthilkumar/dev/personal/wiki-forcast/ajay/meta_data_utils.py:54: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df['end_of_day_size'] = meta_df['end_of_day_size'].replace(0, pd.NA).ffill()\n",
      "/Users/ajaykarthicksenthilkumar/dev/personal/wiki-forcast/ajay/meta_data_utils.py:54: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df['end_of_day_size'] = meta_df['end_of_day_size'].replace(0, pd.NA).ffill()\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [07:49<00:00, 23.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            total_edits  total_bytes_added  unique_editors  mobile edit  \\\n",
      "date                                                                      \n",
      "2015-07-01       3179.0           351572.0          2072.0        475.0   \n",
      "2015-07-02       2898.0            58410.0          1879.0        437.0   \n",
      "2015-07-03       3132.0            38202.0          1885.0        503.0   \n",
      "2015-07-04       3058.0            76277.0          1831.0        585.0   \n",
      "2015-07-05       3045.0            37296.0          1954.0        418.0   \n",
      "\n",
      "            mobile web edit  visualeditor  mw-reverted  mobile app edit  \\\n",
      "date                                                                      \n",
      "2015-07-01            410.0         170.0          0.0             65.0   \n",
      "2015-07-02            403.0         137.0          0.0             34.0   \n",
      "2015-07-03            466.0          65.0          0.0             37.0   \n",
      "2015-07-04            532.0         120.0          0.0             53.0   \n",
      "2015-07-05            378.0         129.0          0.0             40.0   \n",
      "\n",
      "            contenttranslation  visualeditor-switched  end_of_day_size  \n",
      "date                                                                    \n",
      "2015-07-01                 0.0                    4.0      412396187.0  \n",
      "2015-07-02                 0.0                    0.0      412441407.0  \n",
      "2015-07-03                 0.0                    0.0      412499771.0  \n",
      "2015-07-04                 0.0                    0.0      412616185.0  \n",
      "2015-07-05                 0.0                    0.0      412741908.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "topic_exog_features = aggregate_meta_data_for_topic(topic_to_page)\n",
    "first_topic = list(topic_exog_features.keys())[0]\n",
    "print(topic_exog_features[first_topic].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bea8d8f-868a-40cc-96d7-4d1c64e0d7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(SARIMAX_SUMMARY_CLUSTER_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcbf271-7747-423a-a5d5-6be79b68e74e",
   "metadata": {},
   "source": [
    "### SARIMAX Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09052bcb-10f6-4908-bdfa-2214daa247b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing SARIMA models for page: 16\n"
     ]
    }
   ],
   "source": [
    "def fit_sarima(series, p, d, q, P, D, Q, s, exog=None):\n",
    "    model = SARIMAX(series, exog=exog, order=(p, d, q), seasonal_order=(P, D, Q, s))\n",
    "    try:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "            fitted_model = model.fit(disp=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Fitting SARIMA({p},{d},{q}) x ({P},{D},{Q},{s}) failed with default start parameters: {e}\")\n",
    "        # Retry with initial parameters set to zero\n",
    "        try:\n",
    "            fitted_model = model.fit(start_params=[0] * (p + q + P + Q))\n",
    "        except Exception as e:\n",
    "            print(f\"Retry fitting SARIMA({p},{d},{q}) x ({P},{D},{Q},{s}) failed: {e}\")\n",
    "            return None\n",
    "    return fitted_model\n",
    "\n",
    "def find_best_sarima_model(time_series, p_values, d_values, q_values, P_values, D_values, Q_values, s, exog=None):\n",
    "    best_aic = np.inf\n",
    "    best_order = None\n",
    "    best_seasonal_order = None\n",
    "    best_model = None\n",
    "\n",
    "    for p in p_values:\n",
    "        for d in d_values:\n",
    "            for q in q_values:\n",
    "                for P in P_values:\n",
    "                    for D in D_values:\n",
    "                        for Q in Q_values:\n",
    "                            try:\n",
    "                                model = fit_sarima(time_series, p, d, q, P, D, Q, s, exog)\n",
    "                                if model is not None and model.aic < best_aic:\n",
    "                                    best_aic = model.aic\n",
    "                                    best_order = (p, d, q)\n",
    "                                    best_seasonal_order = (P, D, Q, s)\n",
    "                                    best_model = model\n",
    "                            except Exception as e:\n",
    "                                print(f\"Failed to fit SARIMA({p},{d},{q}) x ({P},{D},{Q},{s}): {str(e)}\")\n",
    "    \n",
    "    return {\"order\": best_order, \"seasonal_order\": best_seasonal_order, \"model\": best_model, \"aic\": best_aic}\n",
    "\n",
    "def get_time_series_and_exog(data, page_name):\n",
    "    time_series = data[page_name]\n",
    "    time_series = time_series.asfreq('D')\n",
    "    \n",
    "    meta_df = topic_exog_features[page_name]\n",
    "    # Ensure the index is set to 'date'\n",
    "    if meta_df.index.name != 'date':\n",
    "        meta_df.set_index('date', inplace=True)\n",
    "\n",
    "    # Ensure that the exog features are aligned with the time series data\n",
    "    exog = meta_df.reindex(time_series.index).asfreq('D').fillna(0)\n",
    "    \n",
    "    \n",
    "    # # Day of the week\n",
    "    # day_of_week = pd.to_datetime(time_series.index).dayofweek\n",
    "    # exog_dow = pd.get_dummies(day_of_week, prefix='dow').astype(int)\n",
    "    # exog_dow.index = time_series.index\n",
    "    \n",
    "    # # Month of the year\n",
    "    # month_of_year = pd.to_datetime(time_series.index).month\n",
    "    # exog_month = pd.get_dummies(month_of_year, prefix='month').astype(int)\n",
    "    # exog_month.index = time_series.index\n",
    "    \n",
    "    # # Is weekend\n",
    "    # is_weekend = (day_of_week >= 5).astype(int)\n",
    "    # exog_weekend = pd.DataFrame(is_weekend, index=time_series.index, columns=['is_weekend'])\n",
    "    \n",
    "    # # Is holiday\n",
    "    # holidays = calendar().holidays(start=time_series.index.min(), end=time_series.index.max())\n",
    "    # is_holiday = time_series.index.isin(holidays).astype(int)\n",
    "    # exog_holiday = pd.DataFrame(is_holiday, index=time_series.index, columns=['is_holiday'])\n",
    "    \n",
    "    # # Combine all exogenous features\n",
    "    # exog = pd.concat([exog_dow, exog_month, exog_weekend, exog_holiday], axis=1)\n",
    "    \n",
    "    return time_series, exog\n",
    "\n",
    "def train_test_split(series, exog, test_size):\n",
    "    train = series[:-test_size]\n",
    "    test = series[-test_size:]\n",
    "    train_exog = exog[:-test_size]\n",
    "    test_exog = exog[-test_size:]\n",
    "    return train, test, train_exog, test_exog\n",
    "\n",
    "def process_page(page_name, data, date_columns, p_values, d_values, q_values, P_values, D_values, Q_values, S, test_size):\n",
    "    print(f\"Processing SARIMA models for page: {page_name}\")\n",
    "        \n",
    "    # Get the time series data for the page\n",
    "    time_series, exog = get_time_series_and_exog(data, page_name)\n",
    "    \n",
    "    # Skip the current iteration if no data was found\n",
    "    if time_series is None:\n",
    "        return\n",
    "\n",
    "    # Determine if the series is stationary\n",
    "    is_stationary = check_stationarity(time_series)\n",
    "\n",
    "    # Adjust d_values based on stationarity\n",
    "    adjusted_d_values = [0] if is_stationary else d_values\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train_series, test_series, train_exog, test_exog =  train_test_split(time_series, exog, test_size)\n",
    "\n",
    "    # Find the best ARIMA model for the time series\n",
    "    best_model_info = find_best_sarima_model(train_series, p_values, adjusted_d_values, q_values, P_values, D_values, Q_values, S, exog=train_exog)\n",
    "    \n",
    "    print(f\"Best SARIMA model for {page_name}: Order={best_model_info['order']} AIC={best_model_info['aic']:.2f}\")\n",
    "\n",
    "    return best_model_info\n",
    "\n",
    "\n",
    "def process_and_save_models(data, date_columns, p_values, d_values, q_values, P_values, D_values, Q_values, S, test_size):\n",
    "    # Iterate over all unique pages in the DataFrame\n",
    "    for page_name in data.columns:\n",
    "        best_model_info = process_page(page_name, data, date_columns, p_values, d_values, q_values, P_values, D_values, Q_values, S, test_size)\n",
    "        if best_model_info is not None:\n",
    "            # Save each model's information into a separate pickle file\n",
    "            with open(os.path.join(SARIMAX_SUMMARY_CLUSTER_DIR, f'best_agg_model_{page_name}.pkl'), 'wb') as f:\n",
    "                pickle.dump(best_model_info, f)\n",
    "\n",
    "p_values = [0]\n",
    "d_values = [1]\n",
    "q_values = [0]\n",
    "P_values = [1]\n",
    "D_values = [1]\n",
    "Q_values = [0]\n",
    "s = 60\n",
    "\n",
    "test_size = 30\n",
    "process_and_save_models(aggregated_df.iloc[:, :2], date_columns, p_values, d_values, q_values, P_values, D_values, Q_values, s, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6888c2a-1ae9-471d-8f36-856ff06f0b50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23107c6-3e2f-4ad5-8fef-3a391bfa4023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(actual, forecast):\n",
    "    return 100 * np.mean(2 * np.abs(forecast - actual) / (np.abs(actual) + np.abs(forecast)))\n",
    "\n",
    "def check_residuals(model):\n",
    "    residuals = model.resid\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    residuals.plot(ax=ax[0], title=\"Residuals\")\n",
    "    residuals.plot(kind='kde', ax=ax[1], title=\"Density\")\n",
    "    plt.show()\n",
    "    \n",
    "def plot_forecast_vs_actual(train_series, test_series, train_exog, test_exog, model):\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    ax.plot(train_series, label='Train', color='blue', linewidth=1)\n",
    "    ax.plot(test_series, label='Test', color='orange', linewidth=1)\n",
    "    \n",
    "    # In-sample forecast\n",
    "    in_sample_forecast = model.fittedvalues\n",
    "    ax.plot(in_sample_forecast, label='In-sample Forecast', color='green', linestyle='--', linewidth=2)\n",
    "    \n",
    "    # Out-of-sample forecast\n",
    "    forecast = model.get_forecast(steps=len(test_series), exog=test_exog)\n",
    "    forecast_index = pd.date_range(start=test_series.index[0], periods=len(test_series), freq='D')\n",
    "    forecast_series = pd.Series(forecast.predicted_mean, index=forecast_index)\n",
    "    ax.plot(forecast_series, label='Out-of-sample Forecast', color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "    sMAPE_value = smape(test_series, forecast_series)\n",
    "    print(f'sMAPE: {sMAPE_value:.2f}%')\n",
    "    \n",
    "    # Adding titles and labels\n",
    "    ax.set_title('Actual vs Forecasted Values', fontsize=16)\n",
    "    ax.set_xlabel('Date', fontsize=14)\n",
    "    ax.set_ylabel('Page Views', fontsize=14)\n",
    "    \n",
    "    # Adding legend\n",
    "    ax.legend(loc='upper left', fontsize=12)\n",
    "    \n",
    "    # Adding grid for better readability\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Improving the appearance\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_model_forecast(data, date_columns, page_name, test_size):\n",
    "    try:\n",
    "        with open(os.path.join(SARIMAX_CATEGORY_CLUSTER_DIR, f'best_agg_model_{page_name}.pkl'), 'rb') as f:\n",
    "            best_model_info = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print('FileNotFoundError')\n",
    "        return\n",
    "    \n",
    "    model = best_model_info['model']\n",
    "    \n",
    "    time_series, exog = get_time_series_and_exog(data, page_name)\n",
    "    \n",
    "    if time_series is not None:\n",
    "        train_series, test_series, train_exog, test_exog = train_test_split(time_series, exog, test_size)\n",
    "        plot_forecast_vs_actual(train_series, test_series, train_exog, test_exog, model)\n",
    "        check_residuals(model)\n",
    "\n",
    "\n",
    "page_name = 11\n",
    "\n",
    "plot_model_forecast(aggregated_df, date_columns, page_name, test_size=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92207c20-5359-459d-bb5c-765910461cfc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28ff7c4b-fc7e-42b7-b14e-30eb51a1c68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.402"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([\n",
    "    7.69 , \n",
    "    7.3 ,\n",
    "    5.31 ,\n",
    "    11.54 ,\n",
    "    7.78 ,\n",
    "    14.25 ,\n",
    "    17.37 ,\n",
    "    9.17 ,\n",
    "    5.62 ,\n",
    "    6.21 ,\n",
    "    12.92 ,\n",
    "    13.84 ,\n",
    "    10.16 ,\n",
    "    6.39 ,\n",
    "    8.91 ,\n",
    "    7.70,\n",
    "    18.64 ,\n",
    "    18.31 ,\n",
    "    5.73 ,\n",
    "    13.2 \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029c9037-7271-4006-995e-a5fd34766fde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eff8607-dabb-448a-a114-98eccd0fa547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7bae9c-7953-407e-b006-5270e040dcac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
